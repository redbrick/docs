{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"","tags":[]},{"location":"#home","title":"Home","text":"","tags":[]},{"location":"#redbrick-docs","title":"Redbrick Docs","text":"<p>Welcome to Redbrick's documentation. This is to keep up to date information about the technical infrastructure of Redbrick.</p> <p>This is mostly intended for admins, future admins, webmasters, and everybody else who is grumpy and has no life.</p>","tags":[]},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Redbrick Website</li> <li>SSH help</li> <li>Aperture - Redbrick's new fleet of hardware</li> </ul>","tags":[]},{"location":"#webgroup","title":"Webgroup","text":"<p>The webgroup is a subgroup of Redbrick consisting of volunteers who work with the webmaster on a number of projects.</p>","tags":[]},{"location":"#new-admins","title":"New Admins","text":"<p>So, you want to become an admin. Brave of you. Here's some stuff you should probably read:</p> <ul> <li>Becoming an admin</li> <li>Admin Cheatsheet</li> <li>Redbrick System Administrator Policies</li> <li>Abuse at Redbrick, and the committee's stance on it</li> </ul>","tags":[]},{"location":"contact/","title":"Contact Us","text":"","tags":[]},{"location":"contact/#contact-us","title":"Contact Us","text":"<p>If you have any questions or comments, please contact us at elected-admins@redbrick.dcu.ie. Or you can join the Discord server!</p>","tags":[]},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#tag:aperture","title":"aperture","text":"<ul> <li>            About Aperture          </li> <li>            Aperture          </li> <li>            Aperture Images          </li> <li>            Bastion VM          </li> <li>            Chell          </li> <li>            GlaDOS          </li> <li>            Johnson          </li> <li>            MD (HedgeDoc)          </li> <li>            Nomad          </li> <li>            Pastebin          </li> <li>            User VMs          </li> <li>            Wetty          </li> <li>            Wheatley          </li> </ul>"},{"location":"tags/#tag:api","title":"api","text":"<ul> <li>            Admin API          </li> </ul>"},{"location":"tags/#tag:azazel","title":"azazel","text":"<ul> <li>            Azazel          </li> </ul>"},{"location":"tags/#tag:bind","title":"bind","text":"<ul> <li>            Paphos          </li> </ul>"},{"location":"tags/#tag:chell","title":"chell","text":"<ul> <li>            Chell          </li> </ul>"},{"location":"tags/#tag:daedalus","title":"daedalus","text":"<ul> <li>            Icarus          </li> <li>            LDAP          </li> </ul>"},{"location":"tags/#tag:debian","title":"debian","text":"<ul> <li>            Azazel          </li> </ul>"},{"location":"tags/#tag:details","title":"details","text":"<ul> <li>            Aperture          </li> <li>            Azazel          </li> <li>            Chell          </li> <li>            GlaDOS          </li> <li>            Hardcase          </li> <li>            Icarus          </li> <li>            Johnson          </li> <li>            Motherlode          </li> <li>            Paphos          </li> <li>            Wheatley          </li> <li>            Zeus          </li> </ul>"},{"location":"tags/#tag:discord","title":"discord","text":"<ul> <li>            Blockbot          </li> </ul>"},{"location":"tags/#tag:dns","title":"dns","text":"<ul> <li>            Bind (DNS)          </li> <li>            Paphos          </li> </ul>"},{"location":"tags/#tag:docker","title":"docker","text":"<ul> <li>            MD (HedgeDoc)          </li> <li>            Pastebin          </li> <li>            Wetty          </li> <li>            Zeus          </li> </ul>"},{"location":"tags/#tag:exposed","title":"exposed","text":"<ul> <li>            Services Exposed to the Internet          </li> </ul>"},{"location":"tags/#tag:getting-started","title":"getting-started","text":"<ul> <li>            Aperture          </li> </ul>"},{"location":"tags/#tag:glados","title":"glados","text":"<ul> <li>            GlaDOS          </li> </ul>"},{"location":"tags/#tag:gpg","title":"gpg","text":"<ul> <li>            Open Governance Tagging          </li> </ul>"},{"location":"tags/#tag:hardcase","title":"hardcase","text":"<ul> <li>            Hardcase          </li> </ul>"},{"location":"tags/#tag:hardware","title":"hardware","text":"<ul> <li>            About Aperture          </li> <li>            Aperture Images          </li> <li>            Azazel          </li> <li>            Chell          </li> <li>            GlaDOS          </li> <li>            Hardcase          </li> <li>            Icarus          </li> <li>            Johnson          </li> <li>            Motherlode          </li> <li>            Paphos          </li> <li>            Pygmalion          </li> <li>            Wheatley          </li> <li>            Zeus          </li> </ul>"},{"location":"tags/#tag:icarus","title":"icarus","text":"<ul> <li>            Icarus          </li> <li>            LDAP          </li> </ul>"},{"location":"tags/#tag:images","title":"images","text":"<ul> <li>            Aperture Images          </li> </ul>"},{"location":"tags/#tag:ingress","title":"ingress","text":"<ul> <li>            Bastion VM          </li> </ul>"},{"location":"tags/#tag:install","title":"install","text":"<ul> <li>            Aperture Images          </li> </ul>"},{"location":"tags/#tag:johnson","title":"johnson","text":"<ul> <li>            Johnson          </li> </ul>"},{"location":"tags/#tag:ldap","title":"ldap","text":"<ul> <li>            Admin API          </li> <li>            LDAP          </li> </ul>"},{"location":"tags/#tag:libvirt","title":"libvirt","text":"<ul> <li>            Motherlode          </li> </ul>"},{"location":"tags/#tag:login-box","title":"login-box","text":"<ul> <li>            Azazel          </li> <li>            Pygmalion          </li> </ul>"},{"location":"tags/#tag:motherlode","title":"motherlode","text":"<ul> <li>            Motherlode          </li> </ul>"},{"location":"tags/#tag:nixos","title":"nixos","text":"<ul> <li>            Hardcase          </li> <li>            Icarus          </li> <li>            Motherlode          </li> </ul>"},{"location":"tags/#tag:nomad","title":"nomad","text":"<ul> <li>            Bastion VM          </li> <li>            MD (HedgeDoc)          </li> <li>            Nomad          </li> <li>            Pastebin          </li> <li>            User VMs          </li> <li>            Wetty          </li> </ul>"},{"location":"tags/#tag:open-gov","title":"open-gov","text":"<ul> <li>            Open Governance Tagging          </li> </ul>"},{"location":"tags/#tag:paphos","title":"paphos","text":"<ul> <li>            Paphos          </li> </ul>"},{"location":"tags/#tag:powercut","title":"powercut","text":"<ul> <li>            Post-powercut Todo List          </li> </ul>"},{"location":"tags/#tag:pygmalion","title":"pygmalion","text":"<ul> <li>            Pygmalion          </li> </ul>"},{"location":"tags/#tag:qemu","title":"qemu","text":"<ul> <li>            Motherlode          </li> <li>            User VMs          </li> </ul>"},{"location":"tags/#tag:services","title":"services","text":"<ul> <li>            Admin API          </li> <li>            Bastion VM          </li> <li>            Bind (DNS)          </li> <li>            Services Exposed to the Internet          </li> </ul>"},{"location":"tags/#tag:tagging","title":"tagging","text":"<ul> <li>            Open Governance Tagging          </li> </ul>"},{"location":"tags/#tag:todo","title":"todo","text":"<ul> <li>            Post-powercut Todo List          </li> </ul>"},{"location":"tags/#tag:ubuntu","title":"ubuntu","text":"<ul> <li>            Paphos          </li> <li>            Pygmalion          </li> <li>            Zeus          </li> </ul>"},{"location":"tags/#tag:vm","title":"vm","text":"<ul> <li>            Bastion VM          </li> </ul>"},{"location":"tags/#tag:webgroup","title":"webgroup","text":"<ul> <li>            Blockbot          </li> <li>            Contributing to Webgroup          </li> <li>            Webgroup          </li> </ul>"},{"location":"tags/#tag:wheatley","title":"wheatley","text":"<ul> <li>            Wheatley          </li> </ul>"},{"location":"tags/#tag:zeus","title":"zeus","text":"<ul> <li>            Zeus          </li> </ul>"},{"location":"hardware/","title":"Hardware","text":"","tags":[]},{"location":"hardware/#hardware","title":"Hardware","text":"<p>Here is a list of current hardware in Redbrick's suite of servers, switches and other bits.</p>","tags":[]},{"location":"hardware/#login-boxes","title":"Login Boxes","text":"<ul> <li><code>azazel</code></li> <li><code>pygmalion</code></li> </ul>","tags":[]},{"location":"hardware/#nixos-boxes","title":"NixOS Boxes","text":"<ul> <li><code>hardcase</code></li> <li><code>motherlode</code></li> <li> <p><code>icarus</code></p> </li> <li> <p><code>paphos</code></p> </li> <li><code>zeus</code></li> </ul>","tags":[]},{"location":"hardware/#aperture","title":"Aperture","text":"<ul> <li><code>glados</code></li> <li><code>wheatley</code></li> <li><code>chell</code></li> <li><code>johnson</code></li> <li><code>bastion-vm</code></li> </ul>","tags":[]},{"location":"hardware/#network-hardware","title":"Network Hardware","text":"<ul> <li><code>arse</code></li> <li><code>cerberus</code></li> <li><code>mordor</code></li> <li>switches</li> </ul>","tags":[]},{"location":"hardware/azazel/","title":"Azazel","text":"","tags":["login-box","hardware","azazel","details","debian"]},{"location":"hardware/azazel/#azazel","title":"Azazel","text":"","tags":["login-box","hardware","azazel","details","debian"]},{"location":"hardware/azazel/#details","title":"Details","text":"<ul> <li>Type: Dell PowerEdge R515</li> <li>OS: Debian 12 <code>bookworm</code></li> <li>CPU: 2 x AMD Opteron 4180 @ 2.6Ghz</li> <li>RAM: 16GB</li> <li>Storage: Dell PERC H200 Integrated RAID Controller</li> <li>Disks: 2 x 146GB 15,000 RPM SAS in RAID 1</li> <li>DAS: Worf</li> <li>Drives: Internal SATA DVD\u00b1RW</li> <li>Network: 2x Onboard Ethernet</li> </ul>","tags":["login-box","hardware","azazel","details","debian"]},{"location":"hardware/azazel/#where-to-find","title":"Where to Find","text":"<ul> <li>Internal:<ul> <li><code>10.5.0.1</code></li> </ul> </li> <li>External:<ul> <li><code>136.206.15.24</code></li> </ul> </li> </ul>","tags":["login-box","hardware","azazel","details","debian"]},{"location":"hardware/azazel/#services","title":"Services","text":"<ul> <li>primary ssh login box for users (see Logging in)</li> <li>jump-box for admins</li> </ul>","tags":["login-box","hardware","azazel","details","debian"]},{"location":"hardware/paphos/","title":"Paphos","text":"","tags":["hardware","paphos","details","dns","bind","ubuntu"]},{"location":"hardware/paphos/#paphos","title":"Paphos","text":"","tags":["hardware","paphos","details","dns","bind","ubuntu"]},{"location":"hardware/paphos/#details","title":"Details","text":"<ul> <li>Type: Dell PowerEdge R710</li> <li>OS: Ubuntu 14.04.5 LTS \ud83d\ude2d</li> <li>CPU: 2 x Intel Xeon CPU E5620 @ 2.40Ghz</li> <li>RAM: 16GB</li> <li>Drives: Internal SATA DVD\u00b1RW</li> <li>Network: NetXtreme II BCM5709 Gigabit Ethernet</li> </ul>","tags":["hardware","paphos","details","dns","bind","ubuntu"]},{"location":"hardware/paphos/#where-to-find","title":"Where to Find","text":"<ul> <li>Internal:<ul> <li><code>192.168.0.26</code></li> <li><code>192.168.0.4</code></li> <li><code>10.5.0.6</code></li> </ul> </li> <li>External:<ul> <li><code>136.206.15.26</code></li> <li><code>136.206.15.52</code></li> <li><code>136.206.15.53</code></li> <li><code>136.206.15.55</code></li> <li><code>136.206.15.57</code></li> <li><code>136.206.15.58</code></li> <li><code>136.206.15.101</code></li> <li><code>136.206.15.54</code></li> <li><code>136.206.15.74</code></li> <li><code>ns1.redbrick.dcu.ie</code></li> </ul> </li> </ul>","tags":["hardware","paphos","details","dns","bind","ubuntu"]},{"location":"hardware/paphos/#services","title":"Services","text":"<ul> <li>DNS (bind)</li> </ul>","tags":["hardware","paphos","details","dns","bind","ubuntu"]},{"location":"hardware/pygmalion/","title":"Pygmalion","text":"","tags":["login-box","hardware","pygmalion","ubuntu"]},{"location":"hardware/pygmalion/#pygmalion","title":"Pygmalion","text":"","tags":["login-box","hardware","pygmalion","ubuntu"]},{"location":"hardware/pygmalion/#details","title":"Details","text":"<ul> <li>Type: Intel(R) Xeon (R)</li> <li>OS: Debian 12 <code>bookworm</code></li> <li>CPU: 2x Intel (R) Xeon (R) E5620 2.46GHz</li> <li>RAM: 16GB</li> <li>Network: 4x Broadcom Corporation NetXtreme II BCM5709 Gigabit Ethernet</li> </ul>","tags":["login-box","hardware","pygmalion","ubuntu"]},{"location":"hardware/pygmalion/#where-to-find","title":"Where to Find","text":"<ul> <li>Internal:<ul> <li><code>192.168.0.25</code></li> </ul> </li> <li>External:<ul> <li><code>136.206.15.25</code></li> <li><code>pygmalion.redbrick.dcu.ie</code></li> <li><code>pyg.redbrick.dcu.ie</code></li> </ul> </li> </ul>","tags":["login-box","hardware","pygmalion","ubuntu"]},{"location":"hardware/pygmalion/#services","title":"Services","text":"<ul> <li>secondary ssh login box for users (see Logging in)</li> <li>jump-box for admins</li> </ul>","tags":["login-box","hardware","pygmalion","ubuntu"]},{"location":"hardware/zeus/","title":"Zeus","text":"","tags":["hardware","zeus","details","docker","ubuntu"]},{"location":"hardware/zeus/#zeus","title":"Zeus","text":"","tags":["hardware","zeus","details","docker","ubuntu"]},{"location":"hardware/zeus/#details","title":"Details","text":"<ul> <li>Type: Dell PowerEdge R410</li> <li>OS: Ubuntu 18.04</li> <li>CPU: 2x Intel(R) Xeon (R) x5570 @ 2.93 GHz</li> <li>RAM: 32GB</li> <li>Network: 2x NetXtreme II BCM5716 Gigabit Ethernet</li> </ul>","tags":["hardware","zeus","details","docker","ubuntu"]},{"location":"hardware/zeus/#where-to-find","title":"Where to Find","text":"<ul> <li>Internal:<ul> <li><code>192.168.0.131</code></li> </ul> </li> <li>External:<ul> <li><code>136.206.15.31</code></li> </ul> </li> </ul>","tags":["hardware","zeus","details","docker","ubuntu"]},{"location":"hardware/zeus/#services","title":"Services","text":"<ul> <li>Wetty at: wetty.redbrick.dcu.ie</li> <li>Admin API at: api.redbrick.dcu.ie</li> <li>brickbot2</li> <li>Secretary's email generator  at: generator.redbrick.dcu.ie</li> <li>CodiMD at: md.redbrick.dcu.ie</li> <li>all of this is routed through traefik as a reverse proxy</li> </ul>","tags":["hardware","zeus","details","docker","ubuntu"]},{"location":"hardware/aperture/","title":"Aperture","text":"","tags":["aperture","details","getting-started"]},{"location":"hardware/aperture/#aperture","title":"Aperture","text":"","tags":["aperture","details","getting-started"]},{"location":"hardware/aperture/#what-is-aperture","title":"What is Aperture?","text":"<p>It's nothing to do with cameras. See about for more information on the hardware.</p>","tags":["aperture","details","getting-started"]},{"location":"hardware/aperture/#new-admins","title":"New Admins","text":"<p>If you're a new admin, this is a cheat sheet for you. In order to get broadly up to speed and understand the content of these pages, I suggest you read the following:</p> <ul> <li>About</li> <li>Nomad docs, specifically the job specification and managing nomad jobs pages.</li> <li>Consul docs, specifically how it can be used with Nomad.</li> <li>Ansible docs, specifically the playbook</li> </ul>","tags":["aperture","details","getting-started"]},{"location":"hardware/aperture/#faq","title":"FAQ","text":"<p>So, you've hit a problem. Here's some quicklinks to some common problems:</p> <ul> <li>I can't connect to Aperture</li> <li>Ansible is running into an error</li> </ul>","tags":["aperture","details","getting-started"]},{"location":"hardware/aperture/about/","title":"About Aperture","text":"","tags":["aperture","hardware"]},{"location":"hardware/aperture/about/#about-aperture","title":"About Aperture","text":"<p>Aperture is Redbrick's fleet of hardware that was installed in May 2022 by <code>distro</code>, <code>pints</code>, <code>skins</code>, <code>cawnj</code>, <code>ymacomp</code> and <code>arkues</code>.</p> <p>It consists of:</p> <ul> <li>3x Dell R6515 - <code>glados</code>, <code>wheatley</code>, <code>chell</code></li> </ul> CPU RAM Storage AMD 7302P 3GHz, 16C/32T, 128M, 155W, 3200 2x 16GB RDIMM, 3200MT/s Dual Rank 4x 2TB SATA HDDs (hardware RAID) <ul> <li>2x Ubiquiti USW Pro - <code>rivendell</code>, <code>isengard</code></li> <li>1x Ubiquiti UDM Pro - <code>mordor</code></li> </ul>","tags":["aperture","hardware"]},{"location":"hardware/aperture/about/#servers","title":"Servers","text":"<p>The three servers are named <code>glados</code> , <code>wheatley</code> and <code>chell</code>.</p>","tags":["aperture","hardware"]},{"location":"hardware/aperture/about/#networks","title":"Networks","text":"<p>The firewall is called <code>mordor</code>, and the two 24-port switches are called <code>rivendell</code> and <code>isengard</code>.</p>","tags":["aperture","hardware"]},{"location":"hardware/aperture/about/#networking","title":"Networking","text":"<p>The IP address range for the <code>aperture</code> subnet is <code>10.10.0.0/24</code>, with <code>10.10.0.0/16</code> being used for user VMs.</p> Hostname Internal Address External Address Purpose <code>mordor</code> 10.10.0.1 N/A Firewall <code>rivendell</code> 10.10.0.2 N/A Switch <code>isengard</code> 10.10.0.3 N/A Switch <code>glados</code> 10.10.0.4 136.206.16.4 Server <code>wheatley</code> 10.10.0.5 136.206.16.5 Server <code>chell</code> 10.10.0.6 136.206.16.6 Server <p>Note!</p> <p>Blue cables are used for production network.</p>","tags":["aperture","hardware"]},{"location":"hardware/aperture/about/#kvm","title":"KVM","text":"<p><code>nexus</code> is the name of the KVM switch. It's internal IP address is <code>10.10.0.10</code>.</p> <p><code>glados</code> is connected on port 1, <code>wheatley</code> on port 2, and <code>chell</code> on port 3.</p> <p>Note!</p> <p>Yellow cables are used for KVM network.</p>","tags":["aperture","hardware"]},{"location":"hardware/aperture/about/#idrac","title":"IDRAC","text":"<p>The new servers are all equipped with IDRACs. These still need to be configured.</p> <p>Note!</p> <p>Red cables are used for IDRAC network.</p>","tags":["aperture","hardware"]},{"location":"hardware/aperture/about/#images-click-me","title":"Images (click me)","text":"","tags":["aperture","hardware"]},{"location":"hardware/aperture/about/#switching-from-the-old-network-to-the-new","title":"Switching from the Old Network to the New","text":"<p>We have two address ranges that come in on a single redundant link, so we're exchanging that redundant link for two separate links, each taking responsibility for an address range (<code>136.26.15.0/24</code> and <code>136.206.16.0/24</code>). So we're surrendering redundancy to gain uptime/connectivity during the switchover only. Once the new servers are production ready, we can recombine the link to regain the redundancy.</p>","tags":["aperture","hardware"]},{"location":"hardware/aperture/chell/","title":"Chell","text":"","tags":["hardware","aperture","chell","details"]},{"location":"hardware/aperture/chell/#chell","title":"Chell","text":"","tags":["hardware","aperture","chell","details"]},{"location":"hardware/aperture/chell/#details","title":"Details","text":"<ul> <li>Type: Dell R6515</li> <li>OS: Debian 11</li> <li>CPU: AMD 7302P 3GHz, 16C/32T, 128M, 155W</li> <li>RAM: 2x 16GB RDIMM, 3200MT/s Dual Rank</li> <li>Storage: 4x 2TB SATA HDDs (hardware RAID)</li> </ul> <p>Part of aperture</p>","tags":["hardware","aperture","chell","details"]},{"location":"hardware/aperture/chell/#where-to-find","title":"Where to Find","text":"<ul> <li>Internal:<ul> <li><code>10.10.0.6</code></li> </ul> </li> <li>External:<ul> <li><code>136.206.16.6</code></li> <li><code>chell.redbrick.dcu.ie</code></li> <li><code>chell.aperture.redbrick.dcu.ie</code></li> </ul> </li> </ul>","tags":["hardware","aperture","chell","details"]},{"location":"hardware/aperture/glados/","title":"GlaDOS","text":"","tags":["hardware","aperture","glados","details"]},{"location":"hardware/aperture/glados/#glados","title":"GlaDOS","text":"","tags":["hardware","aperture","glados","details"]},{"location":"hardware/aperture/glados/#details","title":"Details","text":"<ul> <li>Type: Dell R6515</li> <li>OS: Debian 11</li> <li>CPU: AMD 7302P 3GHz, 16C/32T, 128M, 155W</li> <li>RAM: 2x 16GB RDIMM, 3200MT/s Dual Rank</li> <li>Storage: 4x 2TB SATA HDDs (hardware RAID)</li> </ul> <p>Part of aperture</p>","tags":["hardware","aperture","glados","details"]},{"location":"hardware/aperture/glados/#where-to-find","title":"Where to Find","text":"<ul> <li>Internal:<ul> <li><code>10.10.0.4</code></li> </ul> </li> <li>External:<ul> <li><code>136.206.16.4</code></li> <li><code>glados.redbrick.dcu.ie</code></li> <li><code>glados.aperture.redbrick.dcu.ie</code></li> </ul> </li> </ul>","tags":["hardware","aperture","glados","details"]},{"location":"hardware/aperture/images/","title":"Aperture Images","text":"","tags":["hardware","aperture","install","images"]},{"location":"hardware/aperture/images/#aperture-images","title":"Aperture Images","text":"","tags":["hardware","aperture","install","images"]},{"location":"hardware/aperture/images/#servers","title":"Servers","text":"","tags":["hardware","aperture","install","images"]},{"location":"hardware/aperture/images/#networking","title":"Networking","text":"","tags":["hardware","aperture","install","images"]},{"location":"hardware/aperture/images/#some-dancing-for-good-measure","title":"Some Dancing for Good Measure","text":"<p>dancing.mp4</p>","tags":["hardware","aperture","install","images"]},{"location":"hardware/aperture/johnson/","title":"Johnson","text":"","tags":["aperture","hardware","johnson","details"]},{"location":"hardware/aperture/johnson/#johnson","title":"Johnson","text":"","tags":["aperture","hardware","johnson","details"]},{"location":"hardware/aperture/johnson/#details","title":"Details","text":"<p>Formerly <code>albus</code> (in a different life)</p> <ul> <li>Type: Dell PowerEdge R515</li> <li>OS: NixOS</li> <li>CPU: 2 x Opteron 4334 6 core @ 3.2GHz</li> <li>RAM: 32GB</li> <li>Storage: LSI MegaRAID SAS 2108 RAID controller</li> <li>Disks: 2 x 300gb SAS for boot, 8x 1tb SATA ZFS</li> <li>Drives: Internal SATA DVD\u00b1RW</li> <li>Network: 4x Onboard Ethernet, 802.3ad bonding</li> <li>iDRAC NIC: Shared on port 1</li> </ul> <p>Part of aperture</p>","tags":["aperture","hardware","johnson","details"]},{"location":"hardware/aperture/johnson/#where-to-find","title":"Where to Find","text":"<ul> <li>Internal:<ul> <li><code>10.10.0.7</code></li> </ul> </li> <li>2nd NIC is currently unused, would be a good idea to make a bond for more throughput and redundancy on the same ip</li> </ul>","tags":["aperture","hardware","johnson","details"]},{"location":"hardware/aperture/johnson/#services","title":"Services","text":"<ul> <li><code>NFS</code> for aperture</li> </ul>","tags":["aperture","hardware","johnson","details"]},{"location":"hardware/aperture/wheatley/","title":"Wheatley","text":"","tags":["aperture","hardware","wheatley","details"]},{"location":"hardware/aperture/wheatley/#wheatley","title":"Wheatley","text":"","tags":["aperture","hardware","wheatley","details"]},{"location":"hardware/aperture/wheatley/#details","title":"Details","text":"<ul> <li>Type: Dell R6515</li> <li>OS: Debian 11</li> <li>CPU: AMD 7302P 3GHz, 16C/32T, 128M, 155W</li> <li>RAM: 2x 16GB RDIMM, 3200MT/s Dual Rank</li> <li>Storage: 4x 2TB SATA HDDs (hardware RAID)</li> </ul> <p>Part of aperture</p>","tags":["aperture","hardware","wheatley","details"]},{"location":"hardware/aperture/wheatley/#where-to-find","title":"Where to Find","text":"<ul> <li>Internal:<ul> <li><code>10.10.0.5</code></li> </ul> </li> <li>External:<ul> <li><code>136.206.16.5</code></li> <li><code>wheatley.redbrick.dcu.ie</code></li> <li><code>wheatley.aperture.redbrick.dcu.ie</code></li> </ul> </li> </ul>","tags":["aperture","hardware","wheatley","details"]},{"location":"hardware/network/","title":"Redbrick Network Architecture","text":"","tags":[]},{"location":"hardware/network/#redbrick-network-architecture","title":"Redbrick Network Architecture","text":"","tags":[]},{"location":"hardware/network/arse/","title":"Arse","text":"","tags":[]},{"location":"hardware/network/cerberus/","title":"Cerberus (SRX)","text":"","tags":[]},{"location":"hardware/network/mordor/","title":"Mordor","text":"","tags":[]},{"location":"hardware/network/mordor/#mordor","title":"Mordor","text":"","tags":[]},{"location":"hardware/network/mordor/#setup","title":"Setup","text":"<p>The firewall is set up using the personal setup type, using the elected-admins@redbrick.dcu.ie account (stored in <code>pwsafe</code></p> <p>2FA is stored on the same device as the Github 2FA code.</p>","tags":[]},{"location":"hardware/network/mordor/#automatic-updates","title":"Automatic Updates","text":"<p>The UDM Pro is not set up for automatic updates for reliability reasons.</p>","tags":[]},{"location":"hardware/network/mordor/#network-speeds","title":"Network Speeds","text":"<p>We have a 10 GB/s link to DCU's core.</p>","tags":[]},{"location":"hardware/network/mordor/#users","title":"Users","text":"<p>The current elected admins should all have access to the rbadmin account on the firewall. Rootholders should not have access to the firewall unless they are explicitly granted access.</p> <p>The owner account of the unifi equipment is <code>rbadmins</code> (email: elected-admins@redbrick.dcu.ie) with the password stored in <code>pwsafe</code> under <code>unifi</code>.</p> <p>There is a \"super admin\" account that can be used for local access only, details are stored in <code>pwsafe</code> under <code>udmpro-super-admin</code>.</p>","tags":[]},{"location":"hardware/network/mordor/#updates","title":"Updates","text":"<p>The UDM Pro should be kept up to date at all times using the web interface. Please ensure there are no breaking changes before updating.</p> <p>AUTO UPDATES SHOULD NEVER BE ENABLED!</p> <p>This is to prevent a bad update from breaking the UDM Pro and thus the entire network. If you are confident that Unifi can produce stable updates, you may turn it on, however please let the next admins know that you have done this (and update these docs with a comment!).</p>","tags":[]},{"location":"hardware/network/mordor/#advanced-settings","title":"Advanced Settings","text":"<p>SSH is enabled to allow for rollbacks in case of a bad update (I warned you!).</p> <p>Remote access is disabled as it should not be needed, the admin <code>VPN</code> should provide enough access for you. If it is enabled in future, please update these docs with your reasons.</p>","tags":[]},{"location":"hardware/network/mordor/#backups","title":"Backups","text":"<p>Backups are configured to run every week at 1am on a Sunday. 20 backups are stored at a time, therefore storing 20 weeks of configuration. This should be plenty of time to recover from a bad configuration change.</p>","tags":[]},{"location":"hardware/network/mordor/#external-addresses","title":"External Addresses","text":"<p><code>Mordor</code> is NATted when it accesses the Internet. This is because the link address between it and DCU is on a private address.</p> <p>This NATting is used only for the UDM pro device itself, not for the <code>136.206.16.0/24</code> network, and is to allow the UDM box itself to access the Internet.</p> <p>The <code>136.206.16.0/24</code> network is routed down to the UDM pro box, within the DCU network. Essentially there is a route in DCU's network that says \"if you want to access <code>136.206.16.0/24</code> go to <code>mordor</code>\".</p>","tags":[]},{"location":"hardware/network/switches/","title":"switches","text":"","tags":[]},{"location":"hardware/nix/hardcase/","title":"Hardcase","text":"","tags":["nixos","hardware","details","hardcase"]},{"location":"hardware/nix/hardcase/#hardcase","title":"Hardcase","text":"","tags":["nixos","hardware","details","hardcase"]},{"location":"hardware/nix/hardcase/#details","title":"Details","text":"<ul> <li>Type: Dell PowerEdge R410</li> <li>OS: NixOS</li> <li>CPU: 2 x Intel Xeon X5570 @ 2.93GHz</li> <li>RAM: 48GB, incorrectly populated</li> <li>Storage: LSI Logic SAS1068E \"Fake\" RAID controller</li> <li>Disks: 2 x 500GB SATA disks in RAID 1</li> <li>Drives: Internal SATA DVD\u00b1RW</li> <li>Network: 2x Onboard Ethernet, 802.3ad bonding</li> <li>iDRAC NIC: Shared on port 1</li> <li>iDRAC IP is <code>1.158</code></li> </ul>","tags":["nixos","hardware","details","hardcase"]},{"location":"hardware/nix/hardcase/#where-to-find","title":"Where to Find","text":"<ul> <li>Internal:<ul> <li><code>192.168.0.158</code></li> </ul> </li> <li>External:<ul> <li><code>136.206.15.3</code></li> </ul> </li> </ul>","tags":["nixos","hardware","details","hardcase"]},{"location":"hardware/nix/hardcase/#services","title":"Services","text":"<ul> <li>postgreSQL</li> <li>apache</li> <li>monitoring</li> <li>postfix (SMTP)</li> <li>dovecot (IMAP)</li> <li>mailman - mailing lists</li> </ul>","tags":["nixos","hardware","details","hardcase"]},{"location":"hardware/nix/icarus/","title":"Icarus","text":"","tags":["nixos","hardware","icarus","daedalus","details"]},{"location":"hardware/nix/icarus/#icarus","title":"Icarus","text":"<p>Daedalus and Icarus are were twins and thus share documentation.</p> <p>However, Daedalus is now Deadalus and Icarus lives on for now albeit a little sick.</p>","tags":["nixos","hardware","icarus","daedalus","details"]},{"location":"hardware/nix/icarus/#details","title":"Details","text":"<ul> <li>Type: Dell PowerEdge 2950</li> <li>OS: NixOS</li> <li>CPU: 2x Intel Xeon L5335 @ 2.00GHz</li> <li>RAM: 32GB (Daedalus), 16GB (Icarus)</li> <li>Storage: Dell Perc 6/i Integrated RAID controller</li> <li>Disks:<ul> <li>2 x 73GB SAS disks in RAID 1 (hardware)</li> <li>3 x 600GB SAS disks in passthrough (3x RAID 0)</li> </ul> </li> <li>Drives: Internal SATA DVD\u00b1RW</li> <li>Network: 2x Onboard Ethernet, 802.3ad bonding</li> <li>iDRAC NIC: Shared on port 1</li> </ul>","tags":["nixos","hardware","icarus","daedalus","details"]},{"location":"hardware/nix/icarus/#where-to-find","title":"Where to Find","text":"<ul> <li>Internal:<ul> <li><code>192.168.0.150</code></li> </ul> </li> </ul>","tags":["nixos","hardware","icarus","daedalus","details"]},{"location":"hardware/nix/icarus/#services","title":"Services","text":"<ul> <li>LDAP</li> <li>NFS, (a.k.a <code>/storage</code>)</li> <li>GlusterFS, eventually, or some other distributed storage to replace NFS</li> </ul>","tags":["nixos","hardware","icarus","daedalus","details"]},{"location":"hardware/nix/motherlode/","title":"Motherlode","text":"","tags":["nixos","hardware","motherlode","details","qemu","libvirt"]},{"location":"hardware/nix/motherlode/#motherlode","title":"Motherlode","text":"","tags":["nixos","hardware","motherlode","details","qemu","libvirt"]},{"location":"hardware/nix/motherlode/#details","title":"Details","text":"<p>(Something should go here probably)</p>","tags":["nixos","hardware","motherlode","details","qemu","libvirt"]},{"location":"hardware/nix/motherlode/#where-to-find","title":"Where to Find","text":"<ul> <li>Internal:<ul> <li><code>192.168.0.130</code></li> </ul> </li> <li>External:<ul> <li><code>136.206.15.250</code> (dcuclubsandsocs.ie)</li> </ul> </li> </ul>","tags":["nixos","hardware","motherlode","details","qemu","libvirt"]},{"location":"hardware/nix/motherlode/#services","title":"Services","text":"<ul> <li>hosts the VM for dcuclubsandsocs.ie (<code>libvirt</code>/<code>QEMU</code>)</li> </ul>","tags":["nixos","hardware","motherlode","details","qemu","libvirt"]},{"location":"procedures/","title":"Procedures","text":"","tags":[]},{"location":"procedures/#procedures","title":"Procedures","text":"<p>Here you can find a list of various procedures useful for the day-to-day running of Redbrick</p>","tags":[]},{"location":"procedures/#new-elected-admins","title":"New elected admins","text":"","tags":[]},{"location":"procedures/#cheatsheet","title":"Cheatsheet","text":"","tags":[]},{"location":"procedures/#admin-vpn","title":"Admin VPN","text":"","tags":[]},{"location":"procedures/#ansible","title":"Ansible","text":"","tags":[]},{"location":"procedures/#post-powercut-todo-list","title":"Post-powercut Todo List","text":"","tags":[]},{"location":"procedures/#nixos","title":"NixOS","text":"","tags":[]},{"location":"procedures/#updating-wordpress-domains","title":"Updating WordPress Domains","text":"","tags":[]},{"location":"procedures/#irc-ops","title":"IRC Ops","text":"","tags":[]},{"location":"procedures/#committee-handover","title":"Committee Handover","text":"","tags":[]},{"location":"procedures/#redbrick-system-administrator-policies","title":"Redbrick System Administrator Policies","text":"","tags":[]},{"location":"procedures/ansible/","title":"Ansible","text":"","tags":[]},{"location":"procedures/ansible/#ansible","title":"Ansible","text":"<p>Redbrick uses ansible to manage its infrastructure. This document describes the procedures and some tips to get the most out of it.</p>","tags":[]},{"location":"procedures/ansible/#getting-started","title":"Getting Started","text":"","tags":[]},{"location":"procedures/ansible/#installing-ansible","title":"Installing Ansible","text":"<p>Ansible is a python package, so you'll need to install python first. On Debian/Ubuntu, you can do this with:</p> Bash<pre><code>pip install ansible\n</code></pre>","tags":[]},{"location":"procedures/ansible/#add-an-ssh-key","title":"Add an SSH Key","text":"<p>Ansible uses ssh to connect to the remote hosts. You'll need to set up your ssh key so that you can connect to the hosts without constant prompts for passwords.</p>","tags":[]},{"location":"procedures/ansible/#create-a-hosts-file","title":"Create a Hosts File","text":"<p>This is used a phonebook of sorts for ansible. It tells ansible which hosts to connect to, and what user to use.</p> INI<pre><code>[aperture]\nglados\nwheatley\nchell\n\n[aperture:vars]\nansible_user= &lt;your username&gt;\n</code></pre> <p>Contact @distro for a fully populated file.</p>","tags":[]},{"location":"procedures/ansible/#test-it-out","title":"Test it out","text":"Bash<pre><code>ansible all -m ping\n</code></pre> <p>This should connect to all the hosts in the <code>aperture</code> group, and run the <code>ping</code> module. If it works, you're good to go!</p>","tags":[]},{"location":"procedures/ansible/#playbooks","title":"Playbooks","text":"<p>Ansible playbooks are a set of instructions for ansible to run. They're written in YAML, and are usually stored in a file called <code>playbook.yml</code>.</p>","tags":[]},{"location":"procedures/ansible/#writing-a-playbook","title":"Writing a Playbook","text":"<p>Ansible playbooks are written in YAML. The basic structure is:</p> YAML<pre><code>- hosts: &lt;group name&gt;\n  tasks:\n    - name: &lt;task name&gt;\n      &lt;module name&gt;:\n        &lt;module options&gt;\n</code></pre>","tags":[]},{"location":"procedures/ansible/#example","title":"Example","text":"YAML<pre><code>- hosts: aperture\n  tasks:\n    - name: Install curl\n      apt:\n        name: curl\n        state: present\n</code></pre> <p>This playbook will connect to all the hosts in the <code>aperture</code> group, and run the <code>apt</code> module with the <code>name</code> and <code>state</code> options.</p>","tags":[]},{"location":"procedures/ansible/#running-a-playbook","title":"Running a Playbook","text":"Bash<pre><code>ansible-playbook playbook.yml -i hosts\n</code></pre>","tags":[]},{"location":"procedures/ansible/#more-information","title":"More Information","text":"<p>Redbrick's ansible configuration is stored in the ansible folder in the <code>redbrick/nomad</code> repository. There's some more documentation there on each playbook.</p> <p>Ansible's documentation is available here.</p>","tags":[]},{"location":"procedures/ansible/#common-errors","title":"Common Errors","text":"","tags":[]},{"location":"procedures/ansible/#hashicorp-apt-key","title":"Hashicorp Apt Key","text":"<p>Sometimes, when running a playbook, you'll get an error like this:</p> Bash<pre><code>TASK [apt : apt update packages to their latest version and autoclean] ***************************************************************************************************\nfatal: [wheatley]: FAILED! =&gt; {\"changed\": false, \"msg\": \"Failed to update apt cache: unknown reason\"}\nfatal: [chell]: FAILED! =&gt; {\"changed\": false, \"msg\": \"Failed to update apt cache: unknown reason\"}\nfatal: [glados]: FAILED! =&gt; {\"changed\": false, \"msg\": \"Failed to update apt cache: unknown reason\"}\n</code></pre> <p>This is because the Hashicorp apt key has expired. To fix this, uncomment the <code>hashicorp-apt</code> task in the playbook.</p>","tags":[]},{"location":"procedures/cheatsheet/","title":"Cheatsheet","text":"","tags":[]},{"location":"procedures/cheatsheet/#cheatsheet","title":"Cheatsheet","text":"","tags":[]},{"location":"procedures/cheatsheet/#ldap","title":"LDAP","text":"<ul> <li>Query a user</li> </ul> Bash<pre><code>ldapsearch -x uid=\"USERNAME_HERE\"\n</code></pre> <ul> <li>Query user as root for more detailed info</li> </ul> Bash<pre><code>ldapsearch -D \"cn=root,ou=services,o=redbrick\" -y /etc/ldap.secret uid=user\n</code></pre> <ul> <li>Find all users emails created by <code>USERNAME</code></li> </ul> Bash<pre><code>ldapsearch -x createdby=\"user\" uid | awk '/uid:/ {print $2\"@redbrick.dcu.ie\"}'\n</code></pre> <ul> <li>Check if something is backed up on NFS (<code>/storage/path/to/file</code>)</li> </ul> <p>All useful LDAP scripts (edit user quota, reset user password, renew user accounts, etc) are located in the home directory of <code>root</code> on Azazel.</p> <p>Log in as <code>root</code> on a server with local accounts:</p> Bash<pre><code>ssh localaccount@redbrick.dcu.ie\nsudo -i # (same password as localaccount account)\n</code></pre>","tags":[]},{"location":"procedures/cheatsheet/#authenticationpasswords","title":"Authentication/Passwords","text":"","tags":[]},{"location":"procedures/cheatsheet/#onboarding-new-admins","title":"Onboarding New Admins","text":"<ul> <li>Create <code>root</code> ssh key for NixOS Machines Following creation of the key, add to the whitelist in nix configs.</li> </ul> Bash<pre><code>ssh-keygen -t ed25519 # Generate key\ncat ~/.ssh/id_ed25519.pub # Verify it's been created\nssh-copy-id -i ~/.ssh/id_ed25519 user@redbrick.dcu.ie # Copy to local account's ssh dir\nssh -i ~/.ssh/mykey user@redbrick.dcu.ie # Verify that this key was copied\n</code></pre>","tags":[]},{"location":"procedures/cheatsheet/#access-passwordsafe-pwsafe","title":"Access Passwordsafe (pwsafe)","text":"<p>Location of master password vault.</p> <p>Note:</p> <p><code>getpw</code> will prompt you for the Master root password.</p> Bash<pre><code>ssh localroot@halfpint\nsudo -i # to log in as root with local user password\npwsafe # to list passwords\ngetpw &lt;name_of_pass&gt; # Grab password by name key | getpw pygmalion\n</code></pre>","tags":[]},{"location":"procedures/cheatsheet/#ssh-to-root-on-a-nixos-machine","title":"SSH to Root on a NixOS Machine","text":"<ul> <li>From the account you generated your ssh key on (in nix configs) type:</li> </ul> Bash<pre><code>ssh root@hardcase.internal\n</code></pre>","tags":[]},{"location":"procedures/cheatsheet/#nixos","title":"NixOS","text":"<ul> <li>Install a temporary program</li> </ul> Bash<pre><code>nix-shell -p [space seperated package names]\n</code></pre> <ul> <li>Run brickbot2 (running on Metharme)</li> </ul> Bash<pre><code>cd brickbot2\nnix-shell\nsource venv/bin/activate\npython3 main.py config.toml\n</code></pre> <p>Brickbot runs in <code>tmux a -t 0</code> and can be restarted by pressing ctrl+c and running the above python command</p>","tags":[]},{"location":"procedures/cheatsheet/#minecraft-servers","title":"Minecraft Servers","text":"<p>The Redbrick Minecraft server's are dockerized applications running on <code>zeus</code> on a server-per-container basis, using the tools on this GitHub Repo: https://github.com/itzg/docker-minecraft-server#interacting-with-the-server</p> <p>Repo is very well documented so have a look at the README but here's the basics:</p> <p>NOTE: Local Root accounts must be added to the docker group before they can run the docker commands. <code>usermod -a -G docker ACCOUNT_NAME</code></p> <p>You can <code>docker ps | grep minec</code> to find the docker containers running the servers.</p> <p>The docker compose files are located in <code>/etc/docker-compose/services</code>, Unmodded Vanilla compose for example is in <code>/etc/docker-compose/services/minecraft_unmodded/</code></p> <p>To see the configuration for the container you can do <code>docker inspect CONTAINER_NAME_OR_ID</code></p> <ul> <li>Interacting with the Server Console<ul> <li>https://github.com/itzg/docker-minecraft-server#interacting-with-the-server</li> </ul> </li> </ul>","tags":[]},{"location":"procedures/handover/","title":"Handover","text":"","tags":[]},{"location":"procedures/handover/#committee-handover","title":"Committee Handover","text":"<p>When a new committee is elected, there are many things to hand over. This is a list of those things.</p>","tags":[]},{"location":"procedures/handover/#passwords","title":"Passwords","text":"<p>All passwords should be rotated as soon as possible. This is to ensure that passwords are rotated, and that the old committee can no longer access Redbrick using the old passwords. The passwords are stored in Bitwarden, and the master password should be rotated first and foremost.</p>","tags":[]},{"location":"procedures/handover/#2-factor-authentication","title":"2-Factor Authentication","text":"<p>The Chair holds the 2FA key for the Bitwarden account.</p>","tags":[]},{"location":"procedures/irc-ops/","title":"IRC Ops","text":"","tags":[]},{"location":"procedures/irc-ops/#irc-ops","title":"IRC Ops","text":"<p>This is a mirror of:</p> <p>Redbrick cmt Wiki entry</p>","tags":[]},{"location":"procedures/irc-ops/#channel-modes","title":"Channel Modes","text":"<p>It's easy to bugger up the channel with the MODE command, so here's a nice copied and pasted summary of how to use it:</p> <ul> <li><code>/mode {channel} +b {nick|address}</code> - ban somebody by nickname or address mask (nick!account@host)</li> <li><code>/mode {channel} +i</code> - channel is invite-only</li> <li><code>/mode {channel} +l {number}</code> - channel is limited, with {number} users allowed maximal</li> <li><code>/mode {channel} +m</code> - channel is moderated, only chanops and others with 'voice' can <code>talk/mode {channel} +n</code> external <code>/MSG</code>s to channel are not allowed.</li> <li><code>/mode {channel} +p</code> - channel is private</li> <li><code>/mode {channel} +s</code> - channel is secret</li> <li><code>/mode {channel} +t topic</code> - limited, only chanops may change it</li> <li><code>/mode {channel} +o {nick}</code> - makes <code>{nick}</code> a channel operator</li> <li><code>/mode {channel} +v {nick}</code> - gives <code>{nick}</code> a voice</li> </ul>","tags":[]},{"location":"procedures/irc-ops/#other-commands","title":"Other Commands","text":"<p>Basically what you'll be using is:</p> <ul> <li>To kick someone: <code>/kick username</code></li> <li>To ban someone: <code>/mode #lobby +b username</code></li> <li>To set the topic: <code>/topic #lobby whatever</code></li> <li>To op someone: <code>/mode #lobby +o someone</code></li> <li>To op two people: <code>/mode #lobby +oo someone someone_else</code></li> </ul> <p>Or:</p> <ul> <li>To kick someone: <code>/k username</code></li> <li>To ban someone: <code>/ban username</code></li> <li>To unban someone: <code>/unban username</code></li> <li>To set the topic: <code>/t whatever</code></li> <li>To op someone: <code>/op someone</code></li> <li>To op two people: <code>/op someone someone_else</code></li> <li>To deop someone: <code>/deop someone</code></li> </ul>","tags":[]},{"location":"procedures/irc-ops/#sysop-specific-commands","title":"Sysop Specific Commands","text":"<p>These commands can only be run by sysops (i.e. admins in the ircd config file).</p> <ul> <li>Enter BOFH mode (required for all sysop commands): <code>/oper</code></li> <li>Peer to another server*: <code>/sconnect &lt;node name&gt;</code></li> <li>Drop a peer with another server: <code>/squit &lt;node name&gt;</code></li> <li>Force op yourself (do not abuse): <code>/quote opme &lt;channel name&gt;</code></li> <li>Barge into a channel uninvited (again, do not abuse):<code>/quote ojoin #channel</code></li> <li>Barge into a channel uninvited with ops (same again): <code>/quote ojoin @#channel</code></li> <li>Force someone to join a channel: <code>/quote forcejoin nick #channel</code></li> <li>Kill someone: <code>/kill &lt;username&gt; &lt;smartassed kill messsage&gt;</code></li> <li>Ban someone from this server: <code>/kline &lt;username&gt;</code> (there may be more params on this)</li> <li>Ban someone from the entire network: <code>/gline &lt;username&gt;</code> (there may be more params on this)</li> </ul> <p>(thanks to atlas for the quick overview)</p> <ul> <li>Don't try connect to intersocs. Due to crazy endian issues or something they have to connect to us.</li> </ul>","tags":[]},{"location":"procedures/irc-ops/#bots","title":"Bots","text":"<p>It has now become a slight problem with so many bots 'littering' <code>#lobby</code> that anyone wishing to add a new bot to the channel must request permission from the Committee. The main feature wanted is a time limit on bot commands.</p>","tags":[]},{"location":"procedures/irc-ops/#services","title":"Services","text":"<p>The IRC services run by Trinity for all the netsocs. The two services are</p> <p><code>NickServ</code> and <code>ChanServ</code>.</p> <ul> <li><code>/msg NickServ HELP</code></li> <li><code>/msg ChanServ HELP</code></li> </ul> <p>for more details.</p>","tags":[]},{"location":"procedures/new-admins/","title":"New Elected Admins","text":"","tags":[]},{"location":"procedures/new-admins/#new-elected-admins","title":"New Elected Admins","text":"<p>The chronological process of becoming an admin usually looks very similar each year. There are some important things you should know.</p> <p>Remember, being a SysAdmin for the society is not a job, it is a volunteered task you sign up to - don't stress yourself out over it, have fun, and hopefully learn a thing or two.  : )</p>","tags":[]},{"location":"procedures/new-admins/#process","title":"Process","text":"","tags":[]},{"location":"procedures/new-admins/#admin-exam","title":"Admin Exam","text":"<p>Anyone wishing to run and be elected as a SysAdmin must complete a technical exam as an assessment of your knowledge and competency in solving some of the many problems that will be thrown at you.</p> <p>You can find some archives of past exams here, however note that these vary year to year as they are created each year by the currently elected admins.</p>","tags":[]},{"location":"procedures/new-admins/#election-at-agm","title":"Election at AGM","text":"<p>At the annual general meeting, you may nominate yourself, or have someone nominate you to run for SysAdmin. You may only run if you have passed the Admin exam.</p> <p>The amount of admins per year is usually three, to be elected, you must be in the top three voted members.</p>","tags":[]},{"location":"procedures/new-admins/#onboarding","title":"Onboarding","text":"<p>If you are successfully elected - congrats! We welcome you to this pain joy filled journey :)</p> <p>After being elected it is your time to learn the ropes and become familiar with the technicalities of Redbrick.</p> <p>Not alone of course! The previous Admins will assist you on this journey and be there to answer any of your questions, along with this documentation.</p>","tags":[]},{"location":"procedures/nixos/","title":"NixOS","text":"","tags":[]},{"location":"procedures/nixos/#nixos","title":"NixOS","text":"<p>Familiarise yourself with the layout of the following. Bookmarking the page is also a good shout.</p> <p>NixOS documentation</p>","tags":[]},{"location":"procedures/nixos/#who-is-nixos-and-what-does-he-do","title":"Who is NixOS and what Does He Do","text":"<p>NixOS is a distribution of linux that is focused on having a config-first operating system to run services. The advantages of such an approach are the following:</p> <ul> <li>Files dictate how an installation is set up, and as such, can be versioned and tracked in your favourite VCS.</li> <li>New configs can be tested, and safely rolled back.</li> <li>Can be used for both physical and virtual machines in the same way.</li> </ul> <p>Further reading on this can be found on the about page.</p>","tags":[]},{"location":"procedures/nixos/#being-an-admin-nixos-and-you","title":"Being an Admin: NixOS and You","text":"<p>There's a couple of things you'll need to do before you get started with NixOS:</p> <ul> <li>First and foremost is to get set up to contribute to the Redbrick nix-configs repo.</li> </ul> <p>Depending on the powers that be, some sort of normal pr contribution will be acceptable, if you have access a branch is appropriate, in all other cases make a fork and pr back to Redbrick's repo. This will be case by case for those of you reading.</p> <p>Here's a quick hit list of stuff that's worthy of book marking also as you work with Nix:</p> <ul> <li>NixOS Wiki</li> <li>NixOS Manual</li> <li>Nixpkgs index   (unstable means changing, not buggy)</li> <li>Grafana config options   (as an example of how to configure an individual service)</li> </ul> <p>Nix is pretty small as an OS so setting yourself up a node, either as a home server, or as a VM is a solid way to practice how stuff works in an actual environment and lets you work independently of Redbrick. A service you configure at home should be able to run on Redbrick, and vice versa.</p>","tags":[]},{"location":"procedures/nixos/#getting-set-up-to-start-deploying-stuff","title":"Getting Set up to Start Deploying Stuff","text":"<ul> <li> <p>The first step is to navigate to the ssh service config in the nix-config repo here.</p> </li> <li> <p>Make a pull request asking to add the PUBLIC KEY of your ssh key pait to the config file.</p> <ul> <li>The best thing to do is to copy the previous line and modify it to contain your details instead.</li> <li>At time of writing, it is expected for you to generate a <code>ssh-ed25519</code> key. This is subject to change with new cryprographic standards.</li> </ul> </li> <li>Once this is done, contact one of the currently set up users to pull and reload the given machines and you'll have access right away using the accompanying key.</li> </ul>","tags":[]},{"location":"procedures/open-governance-tagging/","title":"Open Governance Tagging","text":"","tags":["open-gov","gpg","tagging"]},{"location":"procedures/open-governance-tagging/#open-governance-tagging-hypnoant-wizzdom","title":"Open Governance Tagging - <code>hypnoant</code>, <code>wizzdom</code>","text":"","tags":["open-gov","gpg","tagging"]},{"location":"procedures/open-governance-tagging/#1-before-the-tagging-ceremony","title":"1. Before the Tagging Ceremony","text":"","tags":["open-gov","gpg","tagging"]},{"location":"procedures/open-governance-tagging/#generating-the-key","title":"Generating the Key","text":"<p>To tag the Open Governance repo you will need to make a new PGP key on the behalf of redbrick committee. Below are the commands and the inputs for creating this key.</p> Bash<pre><code>gpg --full-generate-key\n</code></pre> Key Generation Menu<pre><code>Please select what kind of key you want:\n   (1) RSA and RSA\n   (2) DSA and Elgamal\n   (3) DSA (sign only)\n   (4) RSA (sign only)\n   (9) ECC (sign and encrypt) *default*\n  (10) ECC (sign only)\n  (14) Existing key from card\nYour selection? 1\n\nRSA keys may be between 1024 and 4096 bits long.\nWhat keysize do you want? (3072) 4096\n\nPlease specify how long the key should be valid.\n         0 = key does not expire\n      &lt;n&gt;  = key expires in n days\n      &lt;n&gt;w = key expires in n weeks\n      &lt;n&gt;m = key expires in n months\n      &lt;n&gt;y = key expires in n years\nKey is valid for? (0) {SET FOR DATE AFTER TAGGING CEREMONY}\n\nKey expires at {DATE AFTER TAGGING CEREMONY} IST\nIs this correct? (y/N) y\n\nGnuPG needs to construct a user ID to identify your key.\nReal name: Redbrick Committee\nEmail Address: committee@redbrick.dcu.ie\nComment: Redbrick Committee (Redbrick Open Governance {YEAR-MONTH-TYPE_OF_MEETING(AGM/EGM)})\n\nChange (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? O\n</code></pre>","tags":["open-gov","gpg","tagging"]},{"location":"procedures/open-governance-tagging/#first-sign","title":"First Sign","text":"<p>The signatory who has generated the key will then sign this key.</p> Bash<pre><code>gpg --sign-key {REDBRICK KEY-ID}\n</code></pre> <p>You will then publish this public key to a key-server (e.g. <code>keyserver.ubuntu.com</code> or <code>keys.openpgp.org</code>).</p> Bash<pre><code>gpg --keyserver keyserver.ubuntu.com --send-key committee@redbrick.dcu.ie\n</code></pre>","tags":["open-gov","gpg","tagging"]},{"location":"procedures/open-governance-tagging/#second-sign","title":"Second Sign","text":"<p>The other signatory will pull the key from the key-server and will then sign this key and re-publish the key to the key-server. (You can use the more secure method below for general membership if you wish).</p> Bash<pre><code>gpg --keyserver keyserver.ubuntu.com --recv-key {REDBRICK KEY-ID}\n\ngpg --sign-key {REDBRICK KEY-ID}\n\ngpg --keyserver keyserver.ubuntu.com --send-keys {REDBRICK KEY-ID}\n</code></pre> <p>To verify this procedure has worked and that both signatories have signed it. We will have the first signatory pull the key back down and verify the signatures.</p> Bash<pre><code>gpg --keyserver-options no-self-sigs-only --keyserver keyserver.ubuntu.com --recv-key {REDBRICK KEY-ID}\n</code></pre>","tags":["open-gov","gpg","tagging"]},{"location":"procedures/open-governance-tagging/#general-membership-sign","title":"General Membership Sign","text":"<p>The society now has the option to publish this key to the general membership for them to also sign this key if the current committee wishes to do so. The committee will have to release an email address or another service for the general membership to send files to.</p> <p>Below is the process for a member of the general membership to sign the key.</p> Bash<pre><code>gpg --recv-keys {REDBRICK KEY-ID}\ngpg --sign-key {REDBRICK KEY-ID}\ngpg --armor --export {REDBRICK KEY-ID} | gpg --encrypt -r {REDBRICK KEY-ID} --armor --output {REDBRICK KEY-ID}-signedBy-{OWNER KEY ID}.asc\n</code></pre> <p>They will then send this file to the signatories.</p> <p>The signatories will then use the following commands to import and publish their key with the new signature. This must be done before the</p> Bash<pre><code>gpg -d {REDBRICK KEY-ID}-signedBy-{OWNER KEY ID}.asc  | gpg --import\ngpg --send-key {REDBRICK KEY-ID}\n</code></pre>","tags":["open-gov","gpg","tagging"]},{"location":"procedures/open-governance-tagging/#2-during-the-tagging-ceremony","title":"2. During the Tagging Ceremony","text":"<p>The first signatory shall tag the repository with the following command and styling. There shall be at least 2 witnesses separated by commas.</p> Bash<pre><code>git tag -as {YYYY-MM-TYPEOFMEETING} {COMMIT ID}\n</code></pre> Git Tag Message<pre><code>Co-authored-by: {Signatory 2}\n\nWitnessed-by: ~{WITNESS}\n\nSee `knowledge/tagging.md` for more info.\n</code></pre> <p>They can then push this tag to the GitHub</p> Bash<pre><code>git push --tags origin\n</code></pre>","tags":["open-gov","gpg","tagging"]},{"location":"procedures/open-governance-tagging/#3-after-the-tagging-ceremony","title":"3. After the Tagging Ceremony","text":"","tags":["open-gov","gpg","tagging"]},{"location":"procedures/open-governance-tagging/#verifying-the-tag","title":"Verifying the Tag","text":"<p>Clone the git repository</p> Bash<pre><code>git clone https://github.com/redbrick/open-governance.git\n</code></pre> <p>View the tag</p> Bash<pre><code>git tag -v {YYYY-MM-TYPEOFMEETING}\n</code></pre> <p>Import the key</p> <p>There should be a key signature at the bottom of the tag view. This should be imported into your key-ring. There may be a separate key-server used for the given years key so verify with committee that it is on the correct server for importing.</p> Bash<pre><code>gpg --keyserver-options no-self-sigs-only --keyserver keyserver.ubuntu.com --recv-key {REDBRICK KEY-ID}\n</code></pre> <p>Verify the tag</p> Bash<pre><code>git tag -v {YYYY-MM-TYPEOFMEETING}\n</code></pre> <p>Check the signatories</p> Bash<pre><code>gpg --list-sigs {REDBRICK KEY-ID}\n</code></pre> <p>Import the signatories keys</p> Bash<pre><code>gpg --list-sigs {REDBRICK KEY-ID} --keyid-format long | grep 'ID not found' | perl -nwe '/([0-9A-F]{16})/ &amp;&amp; print \"$1\\n\"' | xargs gpg --keyserver-options no-self-sigs-only --keyserver keyserver.ubuntu.com  --recv-keys\n</code></pre> <p>Export their key</p> Bash<pre><code>gpg --export -a {SIGNATORY KEY-ID}\n</code></pre> <p>Their key should be available at their GitHub under <code>https://github.com/{USERNAME}.gpg</code></p>","tags":["open-gov","gpg","tagging"]},{"location":"procedures/open-governance-tagging/#externally-hosted-repos","title":"Externally Hosted Repos","text":"","tags":["open-gov","gpg","tagging"]},{"location":"procedures/open-governance-tagging/#uploading-the-repo","title":"Uploading the Repo","text":"<ul> <li>First verify that the repo is correctly tagged and signed following the previous steps.</li> <li>Download the zip of the tag from GitHub webpage. (Or clone the repo, checkout the tag and zip the folder)</li> <li>Sign the Zip and verify it:</li> </ul> Bash<pre><code>gpg --sign {NAME OF ZIP}.zip\ngpg --verify {NAME OF ZIP}.zip.gpg\n</code></pre> <ul> <li>Export public key:</li> </ul> Bash<pre><code>gpg --export -a {KEY-ID} &gt; {MYKEYID}\n</code></pre> <ul> <li>Upload the <code>.zip.gpg</code> file and your public key</li> </ul>","tags":["open-gov","gpg","tagging"]},{"location":"procedures/open-governance-tagging/#users-verifying-the-hosted-zip","title":"Users Verifying the Hosted Zip","text":"Bash<pre><code>gpg --import {KEYID}\ngpg --verify {NAME OF ZIP}.zip.gpg\n</code></pre> <ul> <li>Exporting the zip file:</li> </ul> Bash<pre><code>gpg --output {NAME OF ZIP}.zip --decrypt {NAME OF ZIP}.zip.gpg\n</code></pre>","tags":["open-gov","gpg","tagging"]},{"location":"procedures/policies/","title":"Systems Administrator Policies","text":"","tags":[]},{"location":"procedures/policies/#redbrick-system-administrator-policies","title":"Redbrick System Administrator Policies","text":"<p>The purpose of this is to brief new Redbrick system administrators on the current setup, policies and practices in place and to serve as the place to record all such information for current and future administrators.</p>","tags":[]},{"location":"procedures/policies/#admin-account-priviliges","title":"Admin Account Priviliges","text":"<ul> <li>By default, all admin accounts will remain the same as the rest of the committee.</li> <li>Each admin will recieve a local account on each machine that will be in the root group. This allows you to log on if ldap goes down.</li> <li>Accounts should not be placed into any other 'system' or privileged accounts (e.g. pgSQL, mail, news, etc.) but by all accounts (hah, bad pun!) can be placed into useful groups (e.g. cvs, webgroup, helpdesk etc.)</li> </ul>","tags":[]},{"location":"procedures/policies/#root-account","title":"Root account","text":"<p>When su'ing to root, please observe the following:</p> <ul> <li>Wait for the password prompt before typing in the password! Sometimes lag/terminal freezes or whatever can kick in. The other classic mistake is typing the password in place of the username (say for a console login).</li> <li>Make sure LOGNAME is set to your UNIX name. The Linux boxes will prompt you for this. On OpenBSD you can use 'su -m' to keep the environment.</li> <li>Don't change the root account/finger information!</li> <li>If you wish to use another shell, place customisations in your own file. For bash, <code>/root/.bash_profile.&lt;USERNAME&gt;</code> and for zsh <code>/root/.zshrc.&lt;USERNAME&gt;</code>.</li> </ul> <p><code>/root/.zshrc</code> and <code>/root/.bash_profile</code> source in the appropriate file as long as <code>$LOGNAME</code> is set right (see above). Do not put personal customisations into the default root account setup, remember other people have to use it.</p> <p>Common aliases can be put in /root/.profile, familiarise yourself with the existing ones, they can come in handy.</p> <ul> <li>Please keep <code>/root</code> tidy. Don't leave stuff strewn about the place!</li> <li>Make sure to check permissions and ownership on files you work on constantly especially files with important or sensitive information in them (e.g. always use <code>cp -p</code> when copying stuff about).</li> <li>Only use root account when absolutely necessary. Many admin tasks can be done or tested first as a regular user.</li> </ul>","tags":[]},{"location":"procedures/policies/#gotchas","title":"Gotchas","text":"<p>Couple of things to look out for:</p> <ul> <li><code>killall</code> command, never ever use it!</li> <li>Alias <code>cp</code>, <code>mv</code> &amp; <code>rm</code> with the <code>-i</code> option.</li> <li>If you're ever unsure, don't! Ask another admin or check the docs.</li> <li>Always always double check commands before firing them off!</li> </ul>","tags":[]},{"location":"procedures/policies/#admin-mailing-lists","title":"Admin Mailing Lists","text":"<p>lists.redbrick.dcu.ie (<code>Postorius</code>)</p> <ul> <li>All accounts in the root group must be on the admin mailing list and vice versa. Admins who leave/join the root group must be added/removed from the list respectively.</li> <li>Elected Admins should also be on the elected-admins list. This address is mainly used for mail to PayPal, user renewals, registration, and general administration tasks.</li> <li>It is the responsibility of the Elected Admins to ensure that all mailing lists (committee, helpdesk, webmaster, elected-admins, admins, etc) are all up-to-date.</li> </ul>","tags":[]},{"location":"procedures/policies/#admin-account-responsibilities","title":"Admin Account Responsibilities","text":"<p>As an administrator, your new local account has extra privileges (namely being in the root group).</p> <p>For this reason, you should not run any untrusted or unknown programs or scripts.</p> <p>If you must, and source code is available you should check it before running it. Compile your own versions of other user's programs you use regularly. It is far too easy for other users to trojan your account in this manner and get root.</p> <p>Do not use passwordless ssh keys on any of your accounts. When using an untrusted workstation (i.e. just about any PC in DCU!) always check for keyloggers running on the local machine and never use any non system or non personal copies of PuTTY/ssh - there's no way of knowing if they have been trojaned.</p>","tags":[]},{"location":"procedures/policies/#general-responsibilities","title":"General Responsibilities","text":"<p>Look after and regularly monitor all systems, network, hardware and user requests (ones that fall outside of helpdesk's realm, of course!).</p> <p>Actively ensure system and network security. We can't police all user accounts and activities, but basic system security is paramount! Keep up to date with bugtraq/securityfocus etc. Check system logs regularly, process listings, network connections, disk usage, etc.</p>","tags":[]},{"location":"procedures/policies/#downtime","title":"Downtime","text":"<p>All downtime must be scheduled and notified to the members well in advance by means of motd &amp; announce. If it's really important, a mail to announce-redbrick and socials post may be necessary.</p> <p>All unexpected/unscheduled downtime (as a result of a crash or as an emergency precaution) must be explained to the members as soon as possible after the system is brought back. A post to announce, notice in motd or possibly a mail to committee/admins is sufficient.</p> <p>When performing a shutdown, start the shutdown 5 or 10 minutes in advance of the scheduled shutdown time to give people a chance to logout. It may also be useful to disable logins at this stage with a quick explanation in <code>/etc/nologin</code>.</p>","tags":[]},{"location":"procedures/policies/#documentation","title":"Documentation","text":"<p>Please read all the documentation before you do anything, but remember that the docs aren't complete and are sometimes out of date. Please update them as you go :D</p>","tags":[]},{"location":"procedures/post-powercut/","title":"Post-powercut Todo List","text":"","tags":["powercut","todo"]},{"location":"procedures/post-powercut/#post-powercut-todo-list","title":"Post-powercut Todo List","text":"<p>A list of things that should be done/checked immediately after a power cut:</p> <ul> <li>Ensure the <code>aperture</code> servers have the correct IP addresses:<ul> <li><code>eno1</code> should have the internal IP address (<code>10.10.0.0/24</code>) - this should be reserved by DHCP on <code>mordor</code></li> <li><code>eno2</code> should have no IP address</li> <li><code>br0</code> should have the external IP address (<code>136.206.16.0/24</code>) - this should also be reserved by DHCP on <code>mordor</code></li> </ul> </li> <li>If the <code>bastion-vm</code> fails to start, check:<ul> <li><code>/storage</code> is mounted <code>rw</code> on each <code>aperture</code> server</li> <li><code>br0</code> is present and configured on each <code>aperture</code> server</li> <li><code>vm-resources.service.consul</code> is running and <code>http://vm-resources.service.consul:8000/bastion/bastion-vm-latest.qcow2</code> is accessible</li> <li>if the <code>latest</code> symlink points to a corrupted image, <code>ln -sf</code> it to an earlier one</li> </ul> </li> <li>All the <code>nixos</code> boxes rely on <code>DNS</code> for <code>LDAP</code> and <code>NFS</code>:<ul> <li>Make sure bind is running on <code>paphos</code></li> <li>mount <code>/storage</code></li> <li><code>systemctl restart</code> <code>httpd</code>, <code>php-fpm-rbusers-*</code> and <code>ldap</code></li> </ul> </li> <li>Apache on <code>hardcase</code> sometimes tries to start before networking is finished starting. To fix it, disable/re-enable it a few times. This usually makes it turn on.</li> <li>Mailman on <code>hardcase</code> has a lock file at <code>/var/lib/mailman/lock/master.lck</code>. If it doesn't shut down correctly, this lock file will block mailman from starting up. Remove it with:</li> </ul> Bash<pre><code>rm /var/lib/mailman/lock/master.lck\n</code></pre> <ul> <li><code>paphos</code> is old and sometimes its time will become out of sync. To make sure its time is accurate, run:</li> </ul> Bash<pre><code>sudo service ntp restart\n</code></pre> <p>and ensure you have the correct time with <code>date</code></p>","tags":["powercut","todo"]},{"location":"procedures/update-wp-domain/","title":"Update a WordPress Domain","text":"","tags":[]},{"location":"procedures/update-wp-domain/#update-a-wordpress-domain-wizzdom-distro","title":"Update a WordPress Domain - <code>wizzdom</code>, <code>distro</code>","text":"<p>Redbrick hosts a variety of services and websites for various clubs and societies in DCU. Oftentimes these websites hosted for societies run on WordPress due to it's ease of use.</p> <p>However, what happens when you no longer have access to the domain? You can change the domain on the webserver however WordPress will redirect you to the old domain. In this case you must update the database to change the domain. This happened with TheCollegeView in 2023, you can read more about that here</p>","tags":[]},{"location":"procedures/update-wp-domain/#sql-commands","title":"SQL Commands","text":"<p>BACKUPS!!!</p> <p>Ensure you have a recent backup of the database by checking <code>/storage/backups</code></p> <ul> <li>First, check what the current value is</li> </ul> SQL<pre><code>-- validate current setting\nselect option_name,option_value from wp_2options where( option_name=\"siteurl\" or option_name=\"home\");\n</code></pre> <ul> <li>Now, update the option with the new value</li> </ul> SQL<pre><code>-- update to new value\nupdate wp_2options set option_value=\"http://www.thecollegeview.redbrick.dcu.ie\" where( option_name=\"siteurl\" or option_name=\"home\");\n</code></pre> <ul> <li>Verify that the new value is set correctly</li> </ul> SQL<pre><code>-- validate new value\nselect option_name,option_value from wp_2options where( option_name=\"siteurl\" or option_name=\"home\");\n</code></pre> <ul> <li>Now, the same again but for the post content and guid</li> </ul> SQL<pre><code>-- update post content with new domain\nupdate wp_2posts set post_content = replace(post_content,\"://www.thecollegeview.com/\",\"://thecollegeview.redbrick.dcu.ie/\");\n\n-- update the guid with the new domain\nupdate wp_2posts set guid = replace(guid,\"://www.thecollegeview.com/\",\"://thecollegeview.redbrick.dcu.ie/\");\n</code></pre>","tags":[]},{"location":"procedures/vpn/","title":"Admin VPN","text":"","tags":[]},{"location":"procedures/vpn/#admin-vpn","title":"Admin VPN","text":"<p>The admin VPN is set up to allow admins to access the network from outside of DCU, giving them an IP address on the internal network for troubleshooting, testing and integrating.</p> <p>If you just want to create a new client configuration, go here: adding a new client</p>","tags":[]},{"location":"procedures/vpn/#setup","title":"Setup","text":"<p>Installed OpenVPN using this script on <code>glados</code>.</p>","tags":[]},{"location":"procedures/vpn/#adding-a-new-client","title":"Adding a New Client","text":"<p>To add a new client, run the following command (as root) on Glados:</p> Bash<pre><code>bash /root/ovpn/openvpn-install.sh\n</code></pre> <p>You will be prompted to add a new client, enter a name for the client and then the script will generate a new client.</p> <p>It will be saved in <code>/root/[client name].ovpn</code>.</p>","tags":[]},{"location":"procedures/vpn/#revoking-a-client","title":"Revoking a Client","text":"<p>To revoke a client, run the following command (as root) on Glados:</p> Bash<pre><code>bash /root/ovpn/openvpn-install.sh\n</code></pre> <p>You will be prompted to revoke a client, enter the name of the client you want to revoke.</p>","tags":[]},{"location":"procedures/vpn/#connecting-to-the-vpn","title":"Connecting to the VPN","text":"<p>To connect to the VPN, you will need to download the client configuration file from glados and then import it into your OpenVPN client.</p>","tags":[]},{"location":"services/","title":"Services","text":"","tags":[]},{"location":"services/#preface","title":"Preface","text":"<p>Here you will find a list of all the services Redbrick runs, along with some configs and some important information surrounding them.</p> <ul> <li>api</li> <li>bastion-vm</li> <li>bind</li> <li>md</li> <li>consul</li> <li>gitea</li> <li>irc</li> <li>nfs</li> <li>nomad</li> <li>traefik</li> <li>znapzend</li> </ul>","tags":[]},{"location":"services/#adding-more-services","title":"Adding More Services","text":"<p>In order to add a new service, you will need to edit the docs repository.</p> <p>Adding a new service is as easy as creating a new file in <code>docs/services/</code> with an appropriate name, and the page will be automatically added to the navigation pane.</p> <p>Try to keep file names short and concise, limited to one word if possible and avoid using spaces.</p> <p>The style guide for a service file should be as follows:</p> Markdown<pre><code>---\ntitle: ServiceName\nauthor:\n  - username\ntags:\n  - relevant\n  - tags\n\n---\n\n# ServiceName - `username`\n\nShort description on how the service works and where it is running\n\n## Configuration\n\nAdd some possible useful configs here, like a docker-compose file,\ncertain command you may have had to run, or something that is not very obvious.\nLook at other services for hints on this.\n</code></pre>","tags":[]},{"location":"services/api/","title":"Admin API","text":"","tags":["services","api","ldap"]},{"location":"services/api/#redbrick-administrative-web-api","title":"Redbrick Administrative Web API","text":"<p>The source code for the API can be found here.</p> <p>The Redbrick web API serves as an easy interface to carry out administrator tasks (mainly LDAP related), and for use in automation. This saves time instead of accessing machines, and formulating and executing manual LDAP queries or scripts.</p> <p>The server code for the API is hosted on <code>aperture</code> in a docker container deployed with <code>nomad</code>, the job file for which is here. It is written in Python with FastAPI. This container is then served to the public using <code>traefik</code>.</p>","tags":["services","api","ldap"]},{"location":"services/api/#nomad-job-file","title":"Nomad Job File","text":"<p>The nomad job for Redbrick's API is similar to most other web servers for the most part. As always, all secrets are stored in <code>consul</code>. Some things to watch out for are:</p> <ul> <li>The docker image on ghcr.io is private and therefore requires credentials to access.</li> </ul> Nomad<pre><code>auth {\n    username = \"${DOCKER_USER}\"\n    password = \"${DOCKER_PASS}\"\n}\n</code></pre> Nomad<pre><code>template {\n  data        = &lt;&lt;EOH\nDOCKER_USER={{ key \"api/ghcr/username\" }}\nDOCKER_PASS={{ key \"api/ghcr/password\" }}\n...\nEOH\n</code></pre> <ul> <li>The docker container must access <code>/home</code> and <code>/storage</code> on <code>icarus</code> to configure users' home directory and webtree. This is mounted onto the <code>aperture</code> boxes at <code>/oldstorage</code> and is mounted to the containers like this:</li> </ul> Nomad<pre><code>volumes = [\n          \"/oldstorage:/storage\",\n          \"/oldstorage/home:/home\",\n]\n</code></pre> <ul> <li>The container requires the LDAP secret at <code>/etc/ldap.secret</code> to auth with LDAP. This is stored in <code>consul</code>, placed in a template and mounted to the container like this:</li> </ul> Nomad<pre><code>template {\n    destination = \"local/ldap.secret\"\n    data = \"{{ key \\\"api/ldap/secret\\\" }}\" # this is necessary as the secret has no EOF\n    }\n</code></pre> <ul> <li>The container is quite RAM intensive, regularly using <code>700-800MB</code>. The job has been configured to allocate <code>1GB</code> RAM to the container so it does not OOM. The default <code>cpu</code> allocation of <code>300</code> is fine.</li> </ul> Nomad<pre><code>resources {\n    cpu = 300\n    memory = 1024\n    }\n</code></pre>","tags":["services","api","ldap"]},{"location":"services/api/#reference","title":"Reference","text":"<p>For the most up to date, rich API reference please visit https://api.redbrick.dcu.ie/docs</p> <p>All requests are validated with Basic Auth for access. See example.</p> Method Route URL Parameters Body GET /users/<code>username</code> <code>username</code> - Redbrick username N/A PUT /users/<code>username</code> <code>username</code> - Redbrick username <code>ldap_key</code> POST /users/register N/A <code>ldap_value</code>","tags":["services","api","ldap"]},{"location":"services/api/#examples","title":"Examples","text":"<ul> <li><code>GET</code> a user's LDAP data</li> </ul> Python<pre><code>import requests\n\nurl = \"https://api.redbrick.dcu.ie/users/USERNAME_HERE\"\n\nheaders = {\n  'Authorization': 'Basic &lt;ENCODED_USERANDPASS_HERE&gt;'\n}\n\nresponse = requests.request(\"GET\", url, headers=headers)\n\nprint(response.text)\n</code></pre> <ul> <li><code>PUT</code> a user's LDAP data to change their <code>loginShell</code> to <code>/usr/local/shells/zsh</code></li> </ul> Python<pre><code>import requests\nimport json\n\nurl = \"https://api.redbrick.dcu.ie/users/USERNAME_HERE\"\npayload = json.dumps({\n  \"ldap_key\": \"loginShell\",\n  \"ldap_value\": \"/usr/local/shells/zsh\"\n})\nheaders = {\n  'Authorization': 'Basic &lt;ENCODED_USERANDPASS_HERE&gt;',\n  'Content-Type': 'application/json'\n}\n\nresponse = requests.request(\"GET\", url, headers=headers, data=payload)\n\nprint(response.text)\n</code></pre>","tags":["services","api","ldap"]},{"location":"services/api/#important-notes-and-caveats","title":"Important Notes and Caveats","text":"<p>As the FastAPI server for the API is hosted inside of a Docker container, there are limitations to the commands we can execute that affect the \"outside\" world.</p> <p>This is especially important with commands that rely on LDAP.</p> <p>For example inside the <code>ldap-register.sh</code> script used by the <code>/register</code> endpoint.</p> <ul> <li> <p>Commands like <code>chown</code> which require a user group or user to be passed to them will not work because they cannot access these users/groups in the container.</p> </li> <li> <p>This is prevalent in our implementation of the API that creates and modifies users' <code>webtree</code> directory.</p> </li> </ul> <p>How do we fix this?</p> <p>Instead of relying on using users/group names for the <code>chown</code> command, it is advisable to instead use their unique id's.</p> Bash<pre><code># For example, the following commands are equivalent.\nchown USERNAME:member /storage/webtree/U/USERNAME\n\nchown 13371337:103 /storage/webtree/U/USERNAME\n# Where 13371337 is userid and 103 is the id for the 'member' group.\n</code></pre> <p>Note that <code>USERNAME</code> can be used to refer to the user's web directory here since it is the name of the directory and doesn't refer to the user object.</p>","tags":["services","api","ldap"]},{"location":"services/bastion-vm/","title":"Bastion VM","text":"","tags":["aperture","services","nomad","vm","ingress"]},{"location":"services/bastion-vm/#bastion-vm","title":"Bastion VM","text":"<p>This VM is an ephemeral machine that can be placed on any nomad client that has the qemu driver enabled.</p> <p>It acts as the point of ingress for Aperture, with ISS and our mordor allowing traffic to reach it's IP address externally. The VM is configured as a Nomad client itself, in the <code>ingress</code> node pool to ensure that only ingress-type allocations are placed there (like traefik). Those services can proxy requests from the Bastion VM to internal services using consul's service DNS resolution, it's service mesh, or by plain IP and port.</p> <p></p> <p><code>cloud-init</code> is given a static address during the initialisation phase to configure the interface. This ensures that, even if it is replanned, it will be able to accept traffic.</p> <p>The base image that the VM uses is a Debian 12 qcow file. After all configuration was done, the size of the image is <code>~3.2GB</code>. The image can be used to create replicas of the ingress on other external IP addresses, creating more availability if needed.</p>","tags":["aperture","services","nomad","vm","ingress"]},{"location":"services/bastion-vm/#steps-to-deploy","title":"Steps to Deploy","text":"<p>You'll need to ensure the hosts have a bridge device configured to ensure that the networking aspect of the VM can function. See the<code>redbrick/nomad</code> repo for more information about the steps needed for that.</p> <p>You'll need a webserver to serve the <code>cloud-init</code> configs. There may be another solution to this in the near future, but for now, <code>wheatley:/home/mojito/tmp/serve</code> contains the configurations.</p> <p>Plan the Nomad job and wait for the allocation to be created. If you used the correct image (for example a backup of the qcow file) the virtual machine should be configured and should connect as normal to the Consul and Nomad clusters and become eligible for allocations. If you started from scratch, then use the <code>ansible/redbrick-ansible.yml</code> playbook in the <code>redbrick/nomad</code> repo and ensure that the <code>hosts</code> file is up to date.</p> <p>For security's sake, there is no root login and no user accounts on the bastion VM. This is an attempt to make the node more secure. If you need to make changes, you should change the base image and apply that. The less vulnerabilities that are discovered on the bastion VM, the happier we can keep ISS and the safer Redbrick will be.</p>","tags":["aperture","services","nomad","vm","ingress"]},{"location":"services/bind/","title":"Bind (DNS)","text":"","tags":["services","dns"]},{"location":"services/bind/#bind9-distro-ylmcc-wizzdom","title":"Bind9 - <code>distro</code>, <code>ylmcc</code>, <code>wizzdom</code>","text":"<p><code>bind9</code> is our DNS provider. Currently it runs on <code>paphos</code>, but this will change in the near future.</p>","tags":["services","dns"]},{"location":"services/bind/#configuration","title":"Configuration","text":"<p>The config files for bind are located in <code>/etc/bind/master/</code>. The most important files in this directory are:</p> <ul> <li><code>db.Redbrick.dcu.ie</code></li> <li><code>db.Rb.dcu.ie</code></li> <li>various other files for other <code>socs</code> and members</li> </ul> <p>Warning</p> <p>You must never update this file without following the steps below first!</p>","tags":["services","dns"]},{"location":"services/bind/#updating-dns","title":"Updating DNS","text":"<p>To update DNS:</p> <ol> <li>Change directory to <code>/etc/bind/master</code></li> </ol> Bash<pre><code>cd /etc/bind/master\n</code></pre> <ol> <li>Back up the <code>db.Redbrick.dcu.ie</code> file, usually to <code>db.Redbrick.dcu.ie.bak</code></li> </ol> Bash<pre><code>cp db.Redbrick.dcu.ie{,.bak}\n</code></pre> <ol> <li>Stop changes to the file affecting DNS while you edit it</li> </ol> Bash<pre><code>rndc freeze redbrick.dcu.ie\n</code></pre> <ol> <li>Edit <code>db.Redbrick.dcu.ie</code></li> <li>Before changing any DNS entry in the file, you must edit the serial number on 4. You can increment it by one if you want, or follow the format: <code>YYYYMMDDrev</code> where <code>rev</code> is revision. For example:</li> </ol> db.Redbrick.dcu.ie<pre><code>2024033106 ; serial\n</code></pre> <ol> <li>Once you are happy with your file, you can check it with:</li> </ol> Bash<pre><code>named-checkzone redbrick.dcu.ie db.Redbrick.dcu.ie\n</code></pre> <ol> <li>If this returns no errors, you are free to thaw the DNS freeze:</li> </ol> Bash<pre><code>rndc thaw redbrick.dcu.ie\n</code></pre> <ol> <li>Check the status of <code>bind9</code>:</li> </ol> Bash<pre><code>service bind9 status\n</code></pre> <ol> <li>You can access more logs from <code>bind9</code> by checking <code>/var/log/named/default.log</code>:</li> </ol> Bash<pre><code>tail -n 20 /var/log/named/default.log\n</code></pre> <p>Note</p> <p>Once you have verified that everything is working properly. Add your changes and commit them to git.</p>","tags":["services","dns"]},{"location":"services/consul/","title":"Consul","text":"","tags":[]},{"location":"services/consul/#consul","title":"Consul","text":"","tags":[]},{"location":"services/exposed/","title":"Services Exposed to the Internet","text":"","tags":["services","exposed"]},{"location":"services/exposed/#services-exposed-to-the-internet-wizzdom","title":"Services Exposed to the Internet - <code>wizzdom</code>","text":"<p>Firstly, it's important to mention that Redbrick is currently split in 2 parts:</p> <ul> <li>Redbrick 2.0 a.k.a. \"old redbrick\" (on <code>136.206.15.0/24</code>)</li> <li>New Redbrick which includes Aperture (on <code>136.206.16.0/24</code>)</li> </ul> <p></p>","tags":["services","exposed"]},{"location":"services/exposed/#old-redbrick","title":"Old Redbrick","text":"<ul> <li>motherlode - <code>136.206.15.250</code><ul> <li>OS: NixOS 22.05</li> <li>Services:<ul> <li>VM for dcuclubsandsocs.ie</li> </ul> </li> </ul> </li> <li>hardcase - <code>136.206.15.3</code><ul> <li>OS: NixOS 22.05</li> <li>Services:<ul> <li><code>apache httpd</code>:<ul> <li>websites from the webtree (including, but not limited to):<ul> <li>all user's websites <code>&lt;user&gt;.redbrick.dcu.ie</code></li> <li>other websites are mentioned in the nix-configs repo</li> </ul> </li> <li>legacy websites (pretty much anything that isn't dockerized)</li> <li>thecollegeview.ie</li> <li>thelookonline.dcu.ie</li> </ul> </li> <li>email (<code>postfix</code> and <code>dovecot</code>)</li> <li>mailing lists (<code>mailman</code>)</li> <li><code>*.redbrick.dcu.ie</code> also points here</li> </ul> </li> </ul> </li> <li>paphos - <code>136.206.15.53</code><ul> <li>OS: Ubuntu 14.04 LTS</li> <li>Services:<ul> <li>DNS (bind)</li> </ul> </li> </ul> </li> </ul>","tags":["services","exposed"]},{"location":"services/exposed/#new-redbrick","title":"New Redbrick","text":"<ul> <li>azazel - <code>136.206.16.24</code><ul> <li>OS: Debian 12 <code>bookworm</code></li> <li>Services:<ul> <li>primary ssh login box for users (see Logging in)</li> <li>jump-box for admins</li> </ul> </li> </ul> </li> <li>pygmalion - <code>136.206.16.25</code><ul> <li>OS: Debian 12 <code>bookworm</code></li> <li>Services:<ul> <li>secondary ssh login box for users (see Logging in)</li> <li>jump-box for admins</li> </ul> </li> </ul> </li> </ul>","tags":["services","exposed"]},{"location":"services/exposed/#aperture","title":"Aperture","text":"<p>In aperture, things are done a little differently than on the other network. Instead of having a single host per service, aperture is configured to allow services to be allocated dynamically across all 3 servers using nomad, consul and traefik.</p> <ul> <li>glados - <code>136.206.16.4</code></li> <li>wheatley - <code>136.206.16.5</code></li> <li>chell - <code>136.206.16.6</code></li> <li>all 3 boxes are identical</li> <li>OS: Debian 11 <code>bullseye</code></li> <li>Services:<ul> <li>simple <code>nginx</code> containers with the mascot of each server in aperture:<ul> <li>glados</li> <li>wheatley</li> <li>chell</li> </ul> </li> <li>the amikon.me website for DCU AMS in an <code>nginx</code> container</li> <li>timetable.redbrick.dcu.ie a timetable that actually works, 10x better than the official DCU timetable</li> <li>Redbrick main site redbrick.dcu.ie</li> <li>HedgeDoc at: md.redbrick.dcu.ie</li> <li>Admin API at: api.redbrick.dcu.ie</li> <li>Wetty at: wetty.redbrick.dcu.ie</li> <li>DCU Solar Racing Website solarracing.ie</li> <li>Redbrick Password Vault (Vaultwarden) at: vault.redbrick.dcu.ie</li> <li>URL Shortener</li> <li>Plausible Analytics at plausible.redbrick.dcu.ie</li> </ul> </li> <li>Notes:<ul> <li>All web traffic is routed through traefik on the bastion VM</li> <li>All new services will be deployed here</li> <li>Most services here are deployed as docker containers but there's no reason you couldn't use any of the other nomad drivers</li> <li>For more information see redbrick's Nomad repo</li> </ul> </li> </ul>","tags":["services","exposed"]},{"location":"services/gitea/","title":"Gitea","text":"","tags":[]},{"location":"services/gitea/#gitea","title":"Gitea","text":"<p>Redbrick uses Gitea as an open source git host.</p> <ul> <li>Gitea docs</li> <li>Gogs docs, not really important, but Gitea is built on Gogs</li> <li>Link to Redbrick deployment</li> </ul>","tags":[]},{"location":"services/gitea/#deployment","title":"Deployment","text":"<p>Gitea and its database are deployed to Hardcase which runs NixOS</p> <ul> <li>The actual repositories are stored in <code>/zroot/git</code> and most other data is stored in <code>/var/lib/gitea</code></li> <li>The <code>SECRET_KEY</code> and <code>INTERNAL_TOKEN_URI</code> are stored in <code>/var/secrets</code>. They are not automatically created and must be copied when setting up new hosts. Permissions on the <code>gitea_token.secret</code> must be 740 and owned by <code>git:gitea</code></li> <li>Make sure that the <code>gitea_token.secret</code> does NOT have a newline character in it.</li> </ul>","tags":[]},{"location":"services/gitea/#other-notes","title":"Other Notes","text":"<p>The Giteadmin credentials are in the passwordsafe.</p>","tags":[]},{"location":"services/gitea/#operation","title":"Operation","text":"<p>Gitea is very well documented in itself. Here's a couple of special commands when deploying/migrating Gitea to a different host.</p> Bash<pre><code># Regenerate hooks which fixes push errors\n/path/to/gitea admin regenerate hooks\n\n# If you didn't copy the authorized_keys folder then regen that too\n/path/to/gitea admin regenerate keys\n</code></pre>","tags":[]},{"location":"services/icecast/","title":"Icecast","text":"","tags":[]},{"location":"services/icecast/#icecast","title":"Icecast","text":"<p>Icecast is a streaming server that we currently host on aperture.</p> <p>We stream DCUFm's Broadcasts to their apps via a stream presented on <code>dcufm.redbrick.dcu.ie</code>.</p>","tags":[]},{"location":"services/icecast/#procedure","title":"Procedure","text":"<p>The configuration file for icecast is located in the nomad config repo.</p> <p>It should just be a case of running <code>nomad job plan clubs-socs/dcufm.hcl</code> to plan and run the job.</p> <p>Note</p> <p>The job may bind to either the internal or external address. Ensure that if you make a change to the config, you inform DCUfm that they may need to switch which server they use.</p>","tags":[]},{"location":"services/icecast/#streaming-to-icecast","title":"Streaming to Icecast","text":"<p>DCUfm use butt on a desktop in their studio to stream to Icecast.</p> <p>The desktop must be connected to the VPN to ensure the stream stays up, and traefik doesn't reset the connection every 10 seconds. The current icecast configuration for the server is <code>10.10.0.5:2333</code> or <code>136.206.16.5:2333</code> (see above note).</p> <p>Read more about it in this issue.</p> <p>A shortcut to the VPN is available on the desktop (change a shortcut to the binary to include <code>--connect profile.ovpn</code>.</p> <p>See here).</p>","tags":[]},{"location":"services/icecast/#dcufm-cheat-sheet","title":"DCUfm Cheat Sheet","text":"<p>This is a cheat sheet for DCUfm to help them stream to <code>icecast</code>.</p>","tags":[]},{"location":"services/icecast/#connecting-to-the-vpn","title":"Connecting to the VPN","text":"<p>You'll need to connect to the Redbrick VPN to stream to <code>icecast</code>. You can do this by double clicking the shortcut on the desktop.</p> <p>You'll then need to go to bottom right corner of the screen and right click this icon:</p> <p></p> <p>A popup will appear, click connect. This will connect you to the VPN. It may take a second, but a window will pop up with a lot of text. The VPN will connect and then it'll close.</p> <p></p> <p>You should end up with an icon like this:</p> <p></p> <p>You're now connected to the VPN.</p>","tags":[]},{"location":"services/icecast/#connecting-to-icecast","title":"Connecting to Icecast","text":"<p>You'll need to connect to <code>icecast</code> to stream to it. BUTT is the software we use to stream to <code>icecast</code>. You'll also find this on the desktop. Once its open, (and you're connected to the VPN), press the small \"play\" button in the top left corner. This will start your stream to the server.</p> <p>The username and password should already be configured in the software. If not, ask a redbrick sysadmin for the login details.</p> <p>Warning!</p> <p>If you find that butt is not connecting, then you may need to switch which server you're connecting to. To do this, go to settings, and then the \"Main\" tab. In the dropdown, select either DCUfm 1 or DCUfm 2 (try both, one will definitely work).</p>","tags":[]},{"location":"services/icecast/#saving-your-stream","title":"Saving Your Stream","text":"<p>Your stream will be saved automatically onto the desktop into a folder called <code>Recordings YYYY</code> (where <code>YYYY</code> is the current year), with the date and time of the recording, and the format <code>.mp3</code>. Take this file with you (via a USB or similar) if you want to keep it for later, it will not be kept on the desktop for long!</p>","tags":[]},{"location":"services/icecast/#further-information","title":"Further Information","text":"<p>If you have any questions, please ask a redbrick sysadmin.</p>","tags":[]},{"location":"services/irc/","title":"IRC","text":"","tags":[]},{"location":"services/irc/#irc","title":"IRC","text":"","tags":[]},{"location":"services/irc/#redbrick-inspircd","title":"Redbrick InspIRCd","text":"<p>In 2016/2017 we began work to move to InspIRCd. This was due to the complications in ircd-hybrid and how old it was. These complications stopped new netsocs joining us so we all agreed to move irc. $ 4 years later after multiple attempts we had not migrated. Until TCD decided to shutdown their server breaking the network.</p> <p>We run Inspircd v3 on Metharme. InspIRCd's docs can be found here for configuration specifics.</p> <p>IRC is available at <code>irc.redbrick.dcu.ie</code> on port <code>6697</code>. SSL is required for connection, we do not support non-SSL. When connecting from a redbrick server a user will be automatically logged in. If connecting from an external server a user must pass their password on login.</p> <p>For the purpose of external peering of other servers the port <code>7001</code> is exposed as well. Similarly to clients we only support SSL on this port.</p> <p>For docs on connecting and using an IRC client please refer to the wiki.</p>","tags":[]},{"location":"services/irc/#installation","title":"Installation","text":"<p>InspIRCd is installed with Nix. There is no Nix package for InspIRCd so we compile a specific git tag from source. See Nix package for details on how it is compiled. Given we only support SSL and require LDAP, we need to enable both at compile time.</p>","tags":[]},{"location":"services/irc/#configuration","title":"Configuration","text":"<p>InspIRCd's configuration is in Nix here. This config will be converted to xml on disc.</p>","tags":[]},{"location":"services/irc/#important-configuration","title":"Important Configuration","text":"<ul> <li>oper is a list of admin users on the irc server. Their <code>OPER</code> password will need to be manually hashed with <code>hmac-sha256</code>, and placed in a secret on the server to be read in by inspircd.</li> <li>ldapwhitelist is a list of cidr addresses that do no require authentication. The list consists of Redbrick public and private addresses as well as <code>oldsoc</code>.</li> <li>link is a list of all servers we peer with including the anope services server that runs on the same box.</li> </ul>","tags":[]},{"location":"services/irc/#oldsocnet","title":"oldsoc.net","text":"<p><code>oldsoc.net</code> is a server run by old TCD netsocers. All the users on it are the remaining TCD associates following the shutdown of TCD IRCd. This server is maintained by its own users and has explicit permission to join IRC without LDAP auth.</p>","tags":[]},{"location":"services/irc/#anope","title":"Anope","text":"<p>Redbrick runs Anope services for the entire network. As with inspircd we compile from source. Refer to anopes github docs for configuration specifics.</p> <p>Our current Anope is configured with standard mods of chanserv, nickserv and operserv. All config is in here.</p> <p>Anope stores all info in a custom db file on disk.</p>","tags":[]},{"location":"services/irc/#discord-bridge-butlerx","title":"Discord Bridge - <code>butlerx</code>","text":"<p>We run a bridge between the Redbrick Discord and irc. The configuration for this is here.</p> <p>The bridge adds all users from discord with the suffix <code>_d2</code> and all irc users appear as them self but tagged as a bot in discord. Not all discord channels are on IRC, the config above contains a mapping of irc channels to discord channels id's. This needs to be manually updated to add more channels.</p>","tags":[]},{"location":"services/ldap/","title":"LDAP","text":"","tags":["ldap","icarus","daedalus"]},{"location":"services/ldap/#ldap-m1cr0man","title":"LDAP - <code>m1cr0man</code>","text":"<p>LDAP is our directory service. It stores usernames, passwords, UIDs, quotas, and other user specific info.</p> <p>LDAP's structure is different to most other database systems. If you are not familiar with it, I recommend investing some time into looking at how schemas and distinguished names work.</p>","tags":["ldap","icarus","daedalus"]},{"location":"services/ldap/#deployment","title":"Deployment","text":"<ul> <li>OpenLDAP is deployed with Nix to Daedalus and Icarus</li> <li>Daedalus is the master, Icarus is slaved to it and can be used as a read only failover</li> <li><code>ldap.internal</code> and <code>ldap2.internal</code> are slaved to Daedalus + Icarus respectively</li> <li>Both servers store their data in <code>/var/db/openldap</code></li> <li>The ldap.secret, which should ALWAYS have permissions <code>400</code>, and owned by the openldap user, is stored in <code>/var/secrets</code>. It is not automatically created and must be copied when setting up new hosts</li> <li><code>rb-ldap</code> and <code>useradm</code> are wrappers around LDAP that are custom built</li> </ul>","tags":["ldap","icarus","daedalus"]},{"location":"services/ldap/#redbrick-special-notes","title":"Redbrick Special Notes","text":"<ul> <li>The root user password is in the passwordsafe</li> <li>The OID for most of the schema is DCU's</li> <li>The configs that exist for NixOS were mostly ported from our last   LDAP server (<code>paphos</code>) to maintain compatibility</li> <li>At the time of writing, LDAP is not configured with TLS</li> <li>There are 2 scripts to manage quotas on /storage that run on the server serving NFS (<code>zfsquota</code> and <code>zfsquotaquery</code>). They are covered under the NFS documentation.</li> <li>There's a user in ldap called testing, for testing. The password is in <code>pwsafe</code>.</li> </ul>","tags":["ldap","icarus","daedalus"]},{"location":"services/ldap/#operation","title":"Operation","text":"<p>The <code>ldap*</code> suite of commands can be used to manage LDAP. Their man pages are very well documented, but we've provided most common operations below.</p> <p>Note that the ldap.secret is a crypted file, and not equal to the actual password you need to run ldap commands.</p>","tags":["ldap","icarus","daedalus"]},{"location":"services/ldap/#ldapsearch-recipes","title":"Ldapsearch Recipes","text":"<p><code>ldapsearch</code> can be used with and without authenticating as root. Without root, some fields (such as the password hash, altmail) will be hidden.</p> Bash<pre><code># Dump the entire LDAP database in LDIF form, which can be used as a form of backup\nldapsearch -b o=redbrick -xLLL -D cn=root,ou=ldap,o=redbrick -y /path/to/passwd.txt\n\n# Find a user by name, and print their altmail\nldapsearch -b o=redbrick -xLLL -D cn=root,ou=ldap,o=redbrick -y /path/to/passwd.txt uid=m1cr0man altmail\n\n# Find quotas for all users edited by m1cr0man\nldapsearch -b o=redbrick -xLLL updatedby=m1cr0man quota\n\n# Find all member's usernames\nldapsearch -b o=redbrick -xLLL objectClass=member uid\n\n# Find all expired users. Notice here that you can query by hidden fields, but you can't read them\nldapsearch -b o=redbrick -xLLL 'yearsPaid &lt; 1' uid\n</code></pre>","tags":["ldap","icarus","daedalus"]},{"location":"services/ldap/#ldapmodify-recipes","title":"Ldapmodify Recipes","text":"<p>You can instead pass a file with <code>-f</code> when necessary.</p> <p>To test a command add <code>-n</code> for no-op mode.</p> <p>Changing <code>updatedby</code> and <code>updated</code> is added to each command as good practise.</p> Bash<pre><code># Add quota info to a user\nldapmodify -x -D cn=root,ou=ldap,o=redbrick -y /path/to/passwd.txt &lt;&lt; EOF\ndn: uid=testing,ou=accounts,o=redbrick\nchangetype: modify\nadd: quota\nquota: 3G\n-\nreplace: updatedby\nupdatedby: $USER\n-\nreplace: updated\nupdated: $(date +'%F %X')\nEOF\n\n# Change a user's shell\nldapmodify -x -D cn=root,ou=ldap,o=redbrick -y /path/to/passwd.txt &lt;&lt; EOF\ndn: uid=testing,ou=accounts,o=redbrick\nchangetype: modify\nreplace: loginShell\nloginShell: /usr/local/shells/disusered\n-\nreplace: updatedby\nupdatedby: $USER\n-\nreplace: updated\nupdated: $(date +'%F %X')\nEOF\n\n# Update yearsPaid\nldapmodify -x -D cn=root,ou=ldap,o=redbrick -y /path/to/passwd.txt &lt;&lt; EOF\ndn: uid=testing,ou=accounts,o=redbrick\nchangetype: modify\nreplace: yearsPaid\nyearsPaid: 1\n-\nreplace: updatedby\nupdatedby: $USER\n-\nreplace: updated\nupdated: $(date +'%F %X')\nEOF\n</code></pre>","tags":["ldap","icarus","daedalus"]},{"location":"services/ldap/#ldapadd-recipes","title":"Ldapadd Recipes","text":"<p>Occasionally you'll need to add people or things to ldap manually, such as a user you're recreating from backups, or a reserved system name such as a new machine. This is where ldapadd comes in.</p> Bash<pre><code># Create a file to read the new entry from\ncat &gt; add.ldif &lt;&lt; EOF\ndn: uid=redbrick,ou=reserved,o=redbrick\nuid: redbrick\ndescription: DNS entry\nobjectClass: reserved\nobjectClass: top\nEOF\n\n# Import the ldif\nldapadd -x -D cn=root,ou=ldap,o=redbrick -y /path/to/passwd.txt -f add.ldif\n\n# Note if you are importing a full ldif onto a new server, use slapadd instead\n# Ensure slapd is not running first\nslapadd -v -l backup.ldif\n</code></pre>","tags":["ldap","icarus","daedalus"]},{"location":"services/ldap/#other-recipes","title":"Other Recipes","text":"<p>On a yearly basis, the <code>yearsPaid</code> fields must be incremented for every users, and last year's newbies need to be not newbies anymore.</p> <p>Remember to take off <code>-n</code> when you are ready to rock.</p> <p>Adding the <code>updated</code> and <code>updatedby</code> fields from above to these queries would be a good idea.</p> Bash<pre><code># Decrement yearsPaid\n# WARNING NOT IDEMPOTENT, RUN ONCE\nldapsearch -b o=redbrick -xLLL -D cn=root,ou=ldap,o=redbrick -y /path/to/passwd.txt objectClass=member yearsPaid |\\\ntee yearsPaid-$(date +'%F').backup.ldif |\\\nawk '/yearsPaid/ { print \"changetype: modify\\nreplace: yearsPaid\\nyearsPaid: \" $2 - 1 } ! /yearsPaid/ {print $0}' |\\\nldapmodify -x -D cn=root,ou=ldap,o=redbrick -y /path/to/passwd.txt -n\n\n# De-newbie last year's users\nldapsearch -b o=redbrick -xLLL -D cn=root,ou=ldap,o=redbrick -y /path/to/passwd.txt newbie=TRUE dn |\\\ntee newbie-$(date +'%F').backup.ldif |\\\nawk '/^dn/ {print $0\"\\nchangetype: modify\\nreplace: newbie\\nnewbie: FALSE\\n\"}' |\\\nldapmodify -x -D cn=root,ou=ldap,o=redbrick -y /path/to/passwd.txt -n\n\n# Set quotas of users without quotas\nldapsearch -b o=redbrick -xLLL '(&amp;(objectClass=posixAccount)(!(quota=*)))' dn |\\\nawk '/^dn/ {print $0\"\\nchangetype: modify\\nadd: quota\\nquota: 2G\\n\"}' |\\\nldapmodify -x -D cn=root,ou=ldap,o=redbrick -y /path/to/passwd.txt -n\n</code></pre>","tags":["ldap","icarus","daedalus"]},{"location":"services/ldap/#troubleshooting","title":"Troubleshooting","text":"<p>First off, it's worth calling out that if you are coming here to find help with a client side issue, chances are the DNS rule applies:</p> <p>It's probably not LDAP</p> <p>With that out of the way, here's some things to check - in order.</p>","tags":["ldap","icarus","daedalus"]},{"location":"services/ldap/#check-reachability-of-ldap","title":"Check Reachability of LDAP","text":"<p>Run from the master and also from the problem client. It should return <code>m1cr0man</code>'s details. If you get an <code>invalid credentials</code> or <code>object not found</code> check that the LDAP auth config hasn't changed. If you get a connection error then restart the service.</p> Bash<pre><code>ldapsearch -h ldap.internal -p 389 -xLLL -b o=redbrick uid=m1cr0man\n</code></pre>","tags":["ldap","icarus","daedalus"]},{"location":"services/ldap/#verify-ldap-can-be-written-to","title":"Verify LDAP Can Be Written to","text":"<p>Get the password from the passwordsafe. Run this from the master.</p> Bash<pre><code>ldapmodify -D cn=root,ou=ldap,o=redbrick -x -y filewithpwd.txt &lt;&lt; EOF\ndn: uid=m1cr0man,ou=accounts,o=redbrick\nchangetype: modify\nreplace: quota\nquota: 3G\nEOF\n</code></pre> <p>Run the command from the first troubleshooting step to verify the value changed.</p> <p>If it fails with an auth issue, triple check your password file (it should contain the plain text password). If it fails with a non-auth issue, then check the service logs.</p>","tags":["ldap","icarus","daedalus"]},{"location":"services/ldap/#enable-debug-logging","title":"Enable Debug Logging","text":"<p>OpenLDAP produces a nice set of logs when the <code>loglevel</code> is not set.</p> <p>Remove <code>loglevel</code> from <code>extraConfig</code> in the Nix config and switch, then run this command to tail the logs:</p> Bash<pre><code>journalctl -fu openldap\n</code></pre>","tags":["ldap","icarus","daedalus"]},{"location":"services/ldap/#re-syncing-secondary-ldap-servers","title":"Re-syncing Secondary LDAP Server(s)","text":"<p>In the event a secondary server becomes out of sync with the master, it can be synced by stopping the server, deleting its database files, then restarting the server. Do this after ensuring that <code>config.redbrick.ldapSlaveTo</code> is set correctly.</p>","tags":["ldap","icarus","daedalus"]},{"location":"services/md/","title":"MD (HedgeDoc)","text":"","tags":["aperture","nomad","docker"]},{"location":"services/md/#hedgedoc-wizzdom","title":"HedgeDoc - <code>wizzdom</code>","text":"<p>HedgeDoc is deployed with nomad on <code>aperture</code> as a docker container. It is accessible through md.redbrick.dcu.ie.</p> <p>HedgeDoc auths against LDAP and its configuration is available here</p> <p>All sensitive variables are stored in the <code>consul</code> KV store.</p> <p>The important points are as follows:</p> <ul> <li>connecting to the database:</li> </ul> Nomad<pre><code>CMD_DB_URL = \"postgres://{{ key \"hedgedoc/db/user\" }}:{{ key \"hedgedoc/db/password\" }}@{{ env \"NOMAD_ADDR_db\" }}/{{ key \"hedgedoc/db/name\" }}\"\n</code></pre> <ul> <li>disabling anonymous users and email signup:</li> </ul> Nomad<pre><code>CMD_ALLOW_EMAIL_REGISTER = \"false\"\nCMD_ALLOW_ANONYMOUS      = \"false\"\nCMD_EMAIL                = \"false\"\n</code></pre> <ul> <li>LDAP configuration:</li> </ul> Nomad<pre><code>CMD_LDAP_URL             = \"{{ key \"hedgedoc/ldap/url\" }}\"\nCMD_LDAP_SEARCHBASE      = \"ou=accounts,o=redbrick\"\nCMD_LDAP_SEARCHFILTER    = \"{{`(uid={{username}})`}}\"\nCMD_LDAP_PROVIDERNAME    = \"Redbrick\"\nCMD_LDAP_USERIDFIELD     = \"uidNumber\"\nCMD_LDAP_USERNAMEFIELD   = \"uid\"\n</code></pre> <p>See the HedgeDoc docs for more info on configuration.</p>","tags":["aperture","nomad","docker"]},{"location":"services/md/#backups","title":"Backups","text":"<p>The HedgeDoc database is backed up periodically by a nomad job, the configuration for which is here.</p> <p>The bulk of this job is this script which:</p> <ul> <li>grabs the <code>alloc_id</code> of the currently running HedgeDoc allocation from nomad</li> <li>execs into the container running <code>pg_dumpall</code> dumping the database into a file with the current date and time</li> <li>if the backup is unsuccessful the script notifies the admins on discord via a webhook.</li> </ul> Bash<pre><code>#!/bin/bash\n\nfile=/storage/backups/nomad/postgres/hedgedoc/postgresql-hedgedoc-$(date +%Y-%m-%d_%H-%M-%S).sql\n\nmkdir -p /storage/backups/nomad/postgres/hedgedoc\n\nalloc_id=$(nomad job status hedgedoc | grep running | tail -n 1 | cut -d \" \" -f 1)\n\njob_name=$(echo ${NOMAD_JOB_NAME} | cut -d \"/\" -f 1)\n\nnomad alloc exec -task hedgedoc-db $alloc_id pg_dumpall -U {{ key \"hedgedoc/db/user\" }} &gt; \"${file}\"\n\nfind /storage/backups/nomad/postgres/hedgedoc/postgresql-hedgedoc* -ctime +3 -exec rm {} \\; || true\n\nif [ -s \"$file\" ]; then # check if file exists and is not empty\n  echo \"Backup successful\"\n  exit 0\nelse\n  rm $file\n  curl -H \"Content-Type: application/json\" -d \\\n  '{\"content\": \"&lt;@&amp;585512338728419341&gt; `PostgreSQL` backup for **'\"${job_name}\"'** has just **FAILED**\\nFile name: `'\"$file\"'`\\nDate: `'\"$(TZ=Europe/Dublin date)\"'`\\nTurn off this script with `nomad job stop '\"${job_name}\"'` \\n\\n## Remember to restart this backup job when fixed!!!\"}' \\\n  {{ key \"postgres/webhook/discord\" }}\nfi\n</code></pre>","tags":["aperture","nomad","docker"]},{"location":"services/nfs/","title":"NFS","text":"","tags":[]},{"location":"services/nfs/#nfs-network-file-storage","title":"NFS / Network File Storage","text":"<p>NFS is used to serve the notorious <code>/storage</code> directory on Icarus to all of Redbrick's machines, which in turn serves <code>/home</code>, <code>/webtree</code> and some other critical folders.</p>","tags":[]},{"location":"services/nfs/#deployment","title":"Deployment","text":"<ul> <li>NFS is deployed with Nix on Icarus</li> <li>It is backed onto the PowerVault MD1200 with all its disk passed through single-drive RAID 0s toallow for setup of ZFS:<ul> <li>1 mirror of 2x 500GB drives</li> <li>1 mirror of 2x 750GB drives</li> <li>1 mirror of 2x 1TB drives</li> <li>Stripe across all the mirrors for 2TB of usable storage</li> <li>1 hot spare 750GB drive</li> </ul> </li> <li>ZFS is configured with compression onand dedup off</li> <li>The ZFS pool is called <code>zbackup</code></li> </ul>","tags":[]},{"location":"services/nfs/#redbrick-special-notes","title":"Redbrick Special Notes","text":"<p>On each machine where <code>/storage</code> is where NFS is mounted, but <code>/home</code> and <code>/webtree</code> are symlinks into there.</p> <p>There are 2 scripts used to control quotas, detailed below.</p> <p>NFS is backed up to Albus via ZnapZend.</p>","tags":[]},{"location":"services/nfs/#zfsquota-and-zfsquotaquery","title":"<code>zfsquota</code> And <code>zfsquotaquery</code>","text":"<p>These are two bash scripts that run as systemd services on Icarus to manage quotas. This is achieved through getting and setting the <code>userquota</code> and <code>userused</code> properties of the ZFS dataset.</p>","tags":[]},{"location":"services/nfs/#zfsquota","title":"Zfsquota","text":"<p>ZFSQuota will read the <code>quota</code> field from LDAP and sync this with the userquota value on the dataset. It is not event driven - it runs on a timer every 3 hours and syncs all LDAP quotas with ZFS. It can be kicked off manually, which is described below. Users with no quota in LDAP will have no quota in <code>/storage</code>, and users who have their quota removed will persist on ZFS.</p> <p>Changing user names has no impact on this since it is synced with <code>uidNumber</code>.</p>","tags":[]},{"location":"services/nfs/#zfsquotaquery","title":"Zfsquotaquery","text":"<p>ZFSQuotaQuery returns the quota and used space of a particular user. This is used to then inform <code>rbquota</code> which provides the data for the MOTD used space report. Both of these scripts are defined and deployed in the Nix config repo. It runs on port 1995/tcp.</p>","tags":[]},{"location":"services/nfs/#operation","title":"Operation","text":"<p>In general, there isn't too much to do with NFS. Below are some commands of interest for checking its status.</p> Bash<pre><code># On the NFS server, list the exported filesystems\nshowmount -e\n\n# Get the real space usage + fragmentation percent from ZFS\nzpool list zbackup\n\n# Check a user's quota\nzpool get userquota@m1cr0man zbackup\nzpool get userused@m1cr0man zbackup\n\n# Delete a quota from ZFS (useful if a user is deleted)\nzpool set userquota@123456=none zbackup\n\n# Get all user quota usage, and sort it by usage\nzfs userspace -o used,name zbackup | sort -h | tee used_space.txt\n\n# Resync quotas (this command will not return until it is finished)\nsystemctl start zfsquota\n\n# Check the status of zfsquotaquery\nsystemctl status zfsquotaquery\n</code></pre>","tags":[]},{"location":"services/nfs/#troubleshooting","title":"Troubleshooting","text":"<p>In the event where clients are unable to read from NFS, your priority should be restoring the NFS server, rather than</p> <p>unmounting NFS from clients. This is because NFS is mounted in <code>hard</code> mode everywhere, meaning that it will block on IO until a request can be fulfilled.</p>","tags":[]},{"location":"services/nfs/#check-the-server","title":"Check The Server","text":"Bash<pre><code># Check the ZFS volume is readable and writable\nls -l /zbackup/home\ntouch /zbackup/testfile\n\n# Check that rpc.mountd, rpc.statd and rpcbind are running and lisening\nss -anlp | grep rpc\n\n# Check the above services for errors (don't worry about blkmap)\nsystemctl status nfs-{server,idmapd,mountd}\njournalctl -fu nfs-server -u nfs-idmapd -u nfs-mountd\n</code></pre>","tags":[]},{"location":"services/nfs/#check-the-client","title":"Check The Client","text":"Bash<pre><code># Check for connection to NFS\nss -atp | grep nfs\n\n# Check the fstab entry\ngrep storage /etc/fstab\n\n# Check if the NFS server port can be reached\ntelnet 192.168.0.150 2049\n# Entering gibberish should cause the connection to close\n\n# Remount read-only\nmount -o remount,ro /storage\n\n# Not much left you can do but remount entirely or reboot\n</code></pre>","tags":[]},{"location":"services/nfs/#rolling-back-or-restoring-a-backup","title":"Rolling Back or Restoring a Backup","text":"<p>See znapzend</p>","tags":[]},{"location":"services/nomad/","title":"Nomad","text":"","tags":["nomad","aperture"]},{"location":"services/nomad/#nomad-distro-wizzdom","title":"Nomad - <code>distro</code>, <code>wizzdom</code>","text":"<p>Adapted from redbrick/nomad README</p>","tags":["nomad","aperture"]},{"location":"services/nomad/#what-is-nomad","title":"What is Nomad?","text":"<p>Good question!</p> <p>Nomad is a simple and flexible scheduler and orchestrator to deploy and manage containers and non-containerized applications - Nomad Docs</p>","tags":["nomad","aperture"]},{"location":"services/nomad/#deploying-a-nomad-job","title":"Deploying a Nomad Job","text":"<p>All Nomad job related configurations are stored in the <code>nomad</code> directory.</p> <p>The terminology used here is explained here. This is required reading.</p> <ul> <li>Install Nomad on your machine here</li> <li>Clone this repo</li> </ul> Bash<pre><code>git clone git@github.com:redbrick/nomad.git\n</code></pre> <ul> <li>Connect to the admin VPN</li> <li>Set the <code>NOMAD_ADDR</code> environment variable:</li> </ul> Bash<pre><code>export NOMAD_ADDR=http://&lt;IP-ADDRESS-OF-HOST&gt;:4646\n</code></pre> <ul> <li>Check you can connect to the nomad cluster:</li> </ul> Bash<pre><code>nomad status\n</code></pre> <ul> <li>You should receive a list back of all jobs, now you are ready to start deploying!</li> </ul> Bash<pre><code>nomad job plan path/to/job/file.hcl\n</code></pre> <p>This will plan the allocations and ensure that what is deployed is the correct version.</p> <p>If you are happy with the deployment, run</p> Bash<pre><code>nomad job run -check-index [id from last command] path/to/job/file.hcl\n</code></pre> <p>This will deploy the planned allocations, and will error if the file changed on disk between the plan and the run.</p> <p>You can shorten this command to just</p> Bash<pre><code>nomad job plan path/to/file.hcl | grep path/to/file.hcl | bash\n</code></pre> <p>This will plan and run the job file without the need for you to copy and paste the check index id. Only use this once you are comfortable with how Nomad places allocations.</p>","tags":["nomad","aperture"]},{"location":"services/nomad/#restart-a-nomad-job","title":"Restart a Nomad Job","text":"<ul> <li>First, stop and purge the currently-running job</li> </ul> Bash<pre><code>nomad job stop -purge name-of-running-job\n</code></pre> <ul> <li>Run a garbage collection of jobs, evaluations, allocations, nodes and reconcile summaries of all registered jobs.</li> </ul> Bash<pre><code>nomad system gc\n\nnomad system reconcile summaries\n\nnomad system gc # (yes, again)\n</code></pre> <ul> <li>Plan and run the job</li> </ul> Bash<pre><code>nomad job plan path/to/job/file.hcl\n\nnomad job run -check-index [id from last command] path/to/job/file.hcl\n</code></pre>","tags":["nomad","aperture"]},{"location":"services/nomad/#exec-into-container","title":"Exec into Container","text":"<p>At times it is necessary to exec into a docker container to complete maintenance, perform tests or change configurations. The syntax to do this on nomad is similar to <code>docker exec</code> with some small additions:</p> Bash<pre><code>nomad alloc exec -i -t -task &lt;task-name&gt; &lt;nomad-alloc-id&gt; &lt;command&gt;\n</code></pre> <p>Where:</p> <ul> <li><code>&lt;task-name&gt;</code> is the name of the task you want to exec into (only needed when there is more than one task in job)</li> <li><code>&lt;nomad-alloc-id&gt;</code> is the id for the currently running allocation, obtained from the web UI, nomad CLI, or nomad API</li> <li><code>&lt;command&gt;</code> is the command you want to run. e.g. <code>sh</code>, <code>rcon-cli</code></li> </ul>","tags":["nomad","aperture"]},{"location":"services/nomad/#cluster-configuration","title":"Cluster Configuration","text":"<p><code>nomad/cluster-config</code> contains configuration relating to the configuration of the cluster including:</p> <ul> <li>Node Pools</li> <li>agent config</li> </ul>","tags":["nomad","aperture"]},{"location":"services/nomad/#node-pools","title":"Node Pools","text":"<p>Node pools are a way to group nodes together into logical groups which jobs can target that can be used to enforce where allocations are placed.</p> <p>e.g. <code>ingress-pool.hcl</code> is a node pool that is used for ingress nodes such as the bastion-vm. Any jobs that are defined to use <code>node_pool = \"ingress\"</code> such as <code>traefik.hcl</code> and <code>gate-proxy.hcl</code> will only be assigned to one of the nodes in the <code>ingress</code> node pool (i.e. the bastion VM)</p>","tags":["nomad","aperture"]},{"location":"services/paste/","title":"Pastebin","text":"","tags":["aperture","nomad","docker"]},{"location":"services/paste/#pastebin-wizzdom","title":"Pastebin - <code>wizzdom</code>","text":"<p>Redbrick currently uses Privatebin as a paste utility accessible at paste.redbrick.dcu.ie and paste.rb.dcu.ie</p>","tags":["aperture","nomad","docker"]},{"location":"services/paste/#privatebin","title":"Privatebin","text":"<p>The Privatebin instance is deployed with nomad on <code>aperture</code>. Its configuration is available here. Privatebin doesn't support full configuration via environment variables but instead uses a <code>conf.php</code> file. This is passed in using nomad templates.</p> <p>All sensitive variables are stored in the <code>consul</code> KV store.</p> <p>The main points are as follows:</p> <ul> <li>configure URL shortener (<code>shlink</code>)</li> </ul> conf.php<pre><code>urlshortener = \"https://s.rb.dcu.ie/rest/v1/short-urls/shorten?apiKey={{ key \"privatebin/shlink/api\" }}&amp;format=txt&amp;longUrl=\"\n</code></pre> <ul> <li>enable file upload, set file size limit and enable compression</li> </ul> conf.php<pre><code>fileupload = true\nsizelimit = 10485760\ncompression = \"zlib\"\n</code></pre> <ul> <li>Connect to PostgreSQL database</li> </ul> conf.php<pre><code>[model]\nclass = Database\n[model_options]\ndsn = \"pgsql:host=postgres.service.consul;dbname={{ key \"privatebin/db/name\" }}\"\ntbl = \"privatebin_\"     ; table prefix\nusr = \"{{ key \"privatebin/db/user\" }}\"\npwd = \"{{ key \"privatebin/db/password\" }}\"\nopt[12] = true    ; PDO::ATTR_PERSISTENT ; use persistent connections - default\n</code></pre>","tags":["aperture","nomad","docker"]},{"location":"services/servers/","title":"Servers","text":"","tags":[]},{"location":"services/servers/#servers","title":"Servers","text":"<p>Redbrick provides two main servers (Azazel and Pygmalion) for it's members to use for various use cases, for example running applications or user programs.</p>","tags":[]},{"location":"services/servers/#entrypoints","title":"Entrypoints","text":"<p>The main login server used in Redbrick is Azazel. You may also log in to Pygmalion if you wish at <code>pyg.redbrick.dcu.ie</code></p> <p>2 Factor Authentication is required to log in to Redbrick servers. This is done via an SSH key and your Redbrick username/password combination. For more information on how to create an SSH key, and configure your account for 2FA, please read below.</p>","tags":[]},{"location":"services/servers/#logging-in","title":"Logging in","text":"<p>You've set up 2FA on your account with an SSH key, right? If not, you really have to, I'm sorry.</p> <p>You can log in using SSH in your command prompt or terminal application of choice with your Redbrick username and password like so:</p> Bash<pre><code>ssh YOUR_USERNAME@redbrick.dcu.ie -i SSH_KEY_LOCATION_PATH\n\n# When prompted for the password, please input your Redbrick account password.\n# NOTE: The \"-i\" flag specifies the location of your private ssh key.\n</code></pre>","tags":[]},{"location":"services/servers/#alternatives","title":"Alternatives","text":"<p>If you are an unbothered king/queen that simply does not mind using a web interface, let me introduce you to wetty.redbrick.dcu.ie. You do not need an SSH key here.</p>","tags":[]},{"location":"services/servers/#logging-in-to-other-servers","title":"Logging in to other Servers","text":"<p>Your home directory is synced (i.e the same) on all public Redbrick servers. Thus the <code>authorized_keys</code> file will be the same on Azazel as it is on Pygmalion, meaning you can log in to <code>pyg.redbrick.dcu.ie</code> too, and so on.</p>","tags":[]},{"location":"services/servers/#setting-up-an-ssh-key","title":"Setting up an SSH Key","text":"<p>Generating an SSH key pair creates two long strings of characters: a public and a private key. You can place the public key on any server, and then connect to the server using an SSH client that has access to the private key.</p> <p>When these keys match up, and your account password is also correct, you are granted authorisation to log in.</p>","tags":[]},{"location":"services/servers/#1-creating-the-key-pair","title":"1. Creating the Key Pair","text":"<p>On your local computer, in the command line of your choice, enter the following command:</p> Bash<pre><code>ssh-keygen -t ed25519\n</code></pre> <p>Expected Output</p> Text Only<pre><code>Generating public/private ed25519 key pair.\n</code></pre>","tags":[]},{"location":"services/servers/#2-providing-some-extra-details","title":"2. Providing Some Extra Details","text":"<p>You will now be prompted with some information and input prompts:</p> <ul> <li>The first prompt will ask where to save the keys.</li> </ul> Text Only<pre><code>Enter file in which to save the key (e.g /home/bob/.ssh/id_ed25519):\n</code></pre> <p>You can simply press ENTER here to save them at the default location (.ssh directory in your home directory). Alternatively you can specify a custom location if you wish.</p> <ul> <li>The second prompt will ask for a new passphrase to protect the key.</li> </ul> Text Only<pre><code>Enter passphrase (empty for no passphrase):\n</code></pre> <p>Here you may protect this key file with a passphrase. This is optional and recommended for security.</p> <p>Note</p> <p>If you do not wish to add a passphrase to save you all that typing, simply press ENTER for the password and confirmation password prompts.</p> <p>The newly generated public key should now be saved in <code>/home/bob/.ssh/id_ed25519.pub</code>. The private key is the same file is at <code>/home/bob/.ssh/id_ed25519</code>. (i.e under the <code>.ssh</code> folder in your user home directory.)</p>","tags":[]},{"location":"services/servers/#note-for-windows-you-heathen","title":"NOTE FOR WINDOWS (you heathen)","text":"<p>This key is saved under .ssh under your User directory. (i.e <code>C:\\Users\\Bob\\.ssh\\id_ed25519</code>)</p>","tags":[]},{"location":"services/servers/#3-copying-the-public-key-to-the-server","title":"3. Copying the Public Key to the Server","text":"<p>In this step we store our public key on the server we intend to log in to. This key will be used against our secret private key to authenticate our login.</p> <p>For the purposes of this tutorial we will be using Pygmalion (<code>pyg.redbrick.dcu.ie</code>) as our server.</p>","tags":[]},{"location":"services/servers/#logging-in-to-wetty","title":"Logging in to Wetty","text":"<p>In order to access the server to actually place our keys in it, we need to log in via Wetty - a shell interface for Pygmalion on the web.</p> <ul> <li>Head to wetty.redbrick.dcu.ie.</li> </ul> <p>You should see this prompt:</p> Text Only<pre><code>pygmalion.redbrick.dcu.ie login:\n</code></pre> <p>Enter your Redbrick username and press ENTER. When prompted, enter your Redbrick password. Forgot either of these?</p>","tags":[]},{"location":"services/servers/#adding-the-key-into-the-authorized_keys-file","title":"Adding the Key into the <code>authorized_keys</code> File","text":"<ul> <li>Add the key</li> </ul> <p>Grab the contents of your public key. You may use the <code>cat filepath</code> command for this:</p> Bash<pre><code>cat /home/bob/.ssh/id_ed25519.pub\n</code></pre> <p>On Wetty, enter the following command in the shell, with <code>YOUR_KEY</code> replaced with your public ssh key.</p> Bash<pre><code>echo \"YOUR_KEY\" &gt;&gt; ~/.ssh/authorized_keys\n</code></pre> <p>This command will append your public key to the end of the <code>authorized_keys</code> file.</p> <p>Note!</p> <p>The speech marks surrounding YOUR_KEY are important!</p>","tags":[]},{"location":"services/servers/#pssst-made-a-mistake","title":"PSSST\u2026 Made a mistake?","text":"Text Only<pre><code>*You can manually edit the authorized_key file in a text editor with the following command to fix any issues:*\n</code></pre> Bash<pre><code>nano ~/.ssh/authorized_keys\n</code></pre> <p>Congratulations! If you've made it this far, you're ready to login now.</p>","tags":[]},{"location":"services/servers/#forgot-your-password","title":"Forgot Your Password?","text":"<p>Contact an admin on our Discord Server or at elected-admins@redbrick.dcu.ie</p>","tags":[]},{"location":"services/socs/","title":"Socs using Redbrick Infrastructure","text":"","tags":[]},{"location":"services/socs/#socs-using-redbrick-infrastructure","title":"Socs Using Redbrick Infrastructure","text":"<ul> <li>MPS/DCUfm - <code>icecast</code></li> <li>MPS/TheCollegeView - TheCollegeView - <code>wordpress</code></li> <li>DCU Style - The Look - <code>wordpress</code></li> <li>DCU Solar Racing - Solarracing.ie</li> <li>DCU Games Society - <code>minecraft</code></li> </ul>","tags":[]},{"location":"services/traefik/","title":"Traefik","text":"","tags":[]},{"location":"services/traefik/#traefik","title":"Traefik","text":"","tags":[]},{"location":"services/user-vms/","title":"User VMs","text":"","tags":["aperture","nomad","qemu"]},{"location":"services/user-vms/#user-vms","title":"User VMs","text":"<p>User VMs are deployed on <code>aperture</code> using nomad's QEMU driver.</p> <p>Each VM is configured with cloud-init. Those configuration files are served by <code>wheatley</code>, but they can be served by any HTTP server.</p>","tags":["aperture","nomad","qemu"]},{"location":"services/user-vms/#setting-up-networking-on-the-host","title":"Setting up Networking on the Host","text":"<p>The host needs to be configured to allow the VMs to communicate with each other. This is done by creating a bridge and adding the VMs to it.</p>","tags":["aperture","nomad","qemu"]},{"location":"services/user-vms/#create-a-bridge","title":"Create a Bridge","text":"<p>To create a bridge that qemu can use to place the guest (VM) onto the same network as the host, follow the instructions listed here for <code>iproute2</code>, summarised below.</p> <p>We need to create a bridge interface on the host.</p> Bash<pre><code>sudo ip link add name br0 type bridge\nsudo ip link set dev br0 up\n</code></pre> <p>We'll be adding a physical interface to this bridge to allow it to communicate with the external (UDM) network.</p> Bash<pre><code>sudo ip link set eno2 master br0\n</code></pre> <p>You'll need to assign an IP address to the bridge interface. This will be used as the default address for the host. You can do this with DHCP or by assigning a static IP address. The best way to do this is to create a DHCP static lease on the UDM for the bridge interface MAC address.</p> <p>Note</p> <p>TODO: Find out why connectivity seems to be lost when the bridge interface receives an address before the physical interface. If connectivity is lost, release the addresses from both the bridge and the physical interface (in that order) with <code>sudo dhclient -v -r &lt;iface&gt;</code> and then run <code>sudo dhclient -v &lt;iface&gt;</code> to assign the bridge interface an address.</p>","tags":["aperture","nomad","qemu"]},{"location":"services/user-vms/#add-the-vms-to-the-bridge","title":"Add the VMs to the Bridge","text":"<p>The configuration of the qemu network options in the job file will create a new tap interface and add it to the bridge and the VM. I advise you for your own sanity to never touch the network options, they will only cause you pain.</p> <p>For others looking, this configuration is specific to QEMU only.</p> Bash<pre><code>qemu-system-x86_64 ... -netdev bridge,id=hn0 -device virtio-net-pci,netdev=hn0,id=nic1\n</code></pre> <p>This will assign the VM an address on the external network. The VM will be able to communicate with the host and other VMs in the network.</p> <p>You must also add <code>allow br0</code> to <code>/etc/qemu/bridge.conf</code> to allow qemu to add the tap interfaces to the bridge. Source</p> <p>The VMs, once connected to the bridge, will be assigned an address via DHCP. You can assign a static IP address to the VMs by adding a DHCP static lease on the UDM for the VMs MAC address. You can get the address of a VM by checking the <code>nomad alloc logs</code> for that VM and searching for <code>ens3</code>.</p> Bash<pre><code>nomad job status distro-vm | grep \"Node ID\" -A 1 | tail -n 1 | cut -d \" \" -f 1\n# &lt;alloc-id&gt;\nnomad alloc logs &lt;alloc-id&gt; | grep -E \"ens3.*global\" | cut -d \"|\" -f 4 | xargs\n# cloud init... ens3: &lt;ip-address&gt; global\n</code></pre>","tags":["aperture","nomad","qemu"]},{"location":"services/user-vms/#configuring-the-vms","title":"Configuring the VMs","text":"<p>The VMs are configured with cloud-init. Their docs are pretty good, so I won't repeat them here. The files can be served by any HTTP server, and the address is placed into the job file in the QEMU options.</p> Nomad<pre><code>...\n        args = [\n          ...\n          \"virtio-net-pci,netdev=hn0,id=nic1,mac=52:54:84:ba:49:22\", # make sure this MAC address is unique!!\n          \"-smbios\",\n          \"type=1,serial=ds=nocloud-net;s=http://136.206.16.5:8000/\",\n        ]\n...\n</code></pre> <p>Here in the args block:</p> <ul> <li>we define that the VM will have a network device using the <code>virtio</code> driver, we pass it an <code>id</code> and a random unique MAC address</li> <li>we tell it to use <code>smbios</code> type 1 and to grab its <code>cloud-init</code> configs from <code>http://136.206.16.5:8000/</code></li> </ul> <p>Note</p> <p>If you're running multiple VMs on the same network make sure to set different MAC addresses for each VM, otherwise you'll have a bad time.</p>","tags":["aperture","nomad","qemu"]},{"location":"services/user-vms/#creating-a-new-vm","title":"Creating a New VM","text":"<p>To create a new VM, you'll need to create a new job file and a cloud-init configuration file. Copy any of the existing job files and modify them to suit your needs. The cloud-init configuration files can be copied and changed based on the user also. Remember to ensure the MAC addresses are unique!</p>","tags":["aperture","nomad","qemu"]},{"location":"services/wetty/","title":"Wetty","text":"","tags":["aperture","nomad","docker"]},{"location":"services/wetty/#wetty-wizzdom","title":"Wetty - <code>wizzdom</code>","text":"<p>Redbrick uses Wetty as our web terminal of choice. It is accessible at wetty.redbrick.dcu.ie, wetty.rb.dcu.ie,term.redbrick.dcu.ie, anyterm.redbrick.dcu.ie and ajaxterm.redbrick.dcu.ie.</p> <p>Why all the different domains? - For legacy reasons!</p> <p>The configuration is located here</p> <p>The configuration for Wetty is pretty straightforward:</p> <ul> <li><code>SSHHOST</code> - the host that Wetty will connect to (one of the Login boxes), defined in <code>consul</code></li> <li><code>SSHPORT</code> - the port used for ssh</li> <li><code>BASE</code> - the base path for Wetty (default is <code>/wetty</code>)<ul> <li>This isn't very well documented but trust the process. It works!!</li> </ul> </li> </ul> Nomad<pre><code>SSHHOST={{ key \"wetty/ssh/host\" }}\nSSHPORT=22\nBASE=/\n</code></pre>","tags":["aperture","nomad","docker"]},{"location":"services/znapzend/","title":"ZnapZend","text":"","tags":[]},{"location":"services/znapzend/#znapzend","title":"ZnapZend","text":"","tags":[]},{"location":"services/znapzend/#overview","title":"Overview","text":"<p>ZnapZend is used to back up the NFS ZFS dataset from our NFS server to Albus.</p> <p>It can also be used to back up other ZFS datasets on other hosts, but at the time of writing NFS is the only thing being backed up this way.</p> <p>ZnapZend runs on the client and sends backups to Albus over SSH using <code>zfs send | zfs receive</code> piping.</p> <p>The backup strategy can be viewed in the NixOS configuration.</p>","tags":[]},{"location":"services/znapzend/#adding-another-backup","title":"Adding Another Backup","text":"<p>There is not much manual configuration to add a host to the ZnapZend backups.</p> <ol> <li>Create an SSH key for the root user with no passphrase on the host you want to send the backups from. Use <code>ssh-keygen -t ed25519</code>.</li> <li>Add this new SSH public key to the rbbackup user's authorized keys on Albus.</li> <li>Try SSHing to <code>rbbackups@albus.internal</code> to load the host key and test the passwordless authentication.</li> <li>Import the znapzend service config on the sending host and configure <code>redbrick.znapzendSourceDataset</code> and <code>redbrick.znapzendDestDataset</code>. Then apply the config.</li> </ol> <p>Note</p> <p>The <code>DestDataset</code> must be unique across all configured backups/servers.</p>","tags":[]},{"location":"services/znapzend/#debugging","title":"Debugging","text":"<p>Znapzend runs at the top of every hour to make backups. You can watch the progress with <code>journalctl -fu znapzend.service</code>.</p> <p>Failures are usually caused by incorrect SSH configuration, so make sure that passwordless auth using the sending host's root SSH key is working.</p>","tags":[]},{"location":"services/znapzend/#rolling-back-nfs","title":"Rolling Back NFS","text":"<p>If the NFS server is online and functional, you do not need to involve Albus to roll back changes, as all the snapshots</p> <p>are kept on Icarus too.</p> <ol> <li>Find the snapshot you want to restore with <code>zfs list -t snapshot</code>.</li> <li>Run <code>zfs rollback $snapshotname</code>.</li> </ol> <p>That's it! These instructions obviously work for backups other than NFS too, should any ever exist.</p>","tags":[]},{"location":"services/znapzend/#restoring-nfs-from-a-backup","title":"Restoring NFS from a Backup","text":"<p>If the NFS server has died or you are creating a copy of it, here's how to pull the dataset from Albus,</p> <ol> <li>On Albus, find the snapshot you want to restore with <code>zfs list -t snapshot</code>.</li> <li>Open a screen/tmux, and copy the snapshot to a dataset in your target ZFS pool with:</li> </ol> <p><code>bash ssh albus zfs send -vRLec $snapshotname | zfs receive $newpool/$datasetname`</code></p>","tags":[]},{"location":"webgroup/","title":"Webgroup","text":"","tags":["webgroup"]},{"location":"webgroup/#webgroup","title":"Webgroup","text":"<p>The webgroup is a subgroup of Redbrick consisting of volunteers who work with the webmaster on a number of projects including but not limited to: Blockbot, the Redbrick website, and various other projects.</p> <p>Read more about how the webgroup operates in open governance.</p>","tags":["webgroup"]},{"location":"webgroup/#webgroup-projects","title":"Webgroup Projects","text":"","tags":["webgroup"]},{"location":"webgroup/#blockbot","title":"Blockbot","text":"<p>A Discord bot, written in Python, that is maintained by the Redbrick Webgroup.</p>","tags":["webgroup"]},{"location":"webgroup/blockbot/","title":"Blockbot","text":"","tags":["webgroup","discord"]},{"location":"webgroup/blockbot/#blockbot","title":"Blockbot","text":"<p>Blockbot is a Discord bot, written in Python, that is maintained by the Redbrick Webgroup. This project uses <code>hikari</code>, an opinionated microframework, to interface with the Discord API. <code>hikari-arc</code> is the command handler and <code>hikari-miru</code> is the component handler of choice.</p>","tags":["webgroup","discord"]},{"location":"webgroup/blockbot/#file-structure","title":"File Structure","text":"<p>All bot files are under <code>src/</code>.</p> <ul> <li><code>bot.py</code><ul> <li>Contains the bot configuration and instantiation (e.g. loading the bot extensions).</li> </ul> </li> <li><code>extensions/</code><ul> <li>Contains extensions (files) that are loaded when the bot is started. Extensions are a way to split bot logic across multiple files, commonly used to group different features. Extensions can contain plugins, command, event listeners and other logic. Read more.</li> </ul> </li> <li><code>examples/</code><ul> <li>Contains example extensions with commands, components and more as reference for developers.</li> </ul> </li> <li><code>config.py</code><ul> <li>Configuration secrets and important constants (such as identifiers) are stored here. The secrets are loaded from environment variables, so you can set them in your shell or in a <code>.env</code> file.</li> </ul> </li> <li><code>database.py</code><ul> <li>Contains database configuration and models. Uses SQLAlchemy</li> </ul> </li> <li><code>hooks.py</code><ul> <li>Contains command hooks - functions which run before command invocation to determine whether or not the command should actually be run (e.g. permissions check).</li> </ul> </li> <li><code>utils.py</code><ul> <li>Utility functions are stored here, that can be reused across the codebase.</li> </ul> </li> </ul>","tags":["webgroup","discord"]},{"location":"webgroup/blockbot/#installation","title":"Installation","text":"<p>Tip</p> <p>Docker Compose for local development is highly recommended. This is similar to how it is deployed on Redbrick.</p>","tags":["webgroup","discord"]},{"location":"webgroup/blockbot/#discord-developer-portal","title":"Discord Developer Portal","text":"<p>As a prerequisite, you need to have an application registered on the Discord developer portal.</p> <ol> <li>Create a Discord application here.</li> <li>Go to \"OAuth2 &gt; URL Generator\" on the left sidebar, select the <code>bot</code> and <code>applications.commands</code> scopes, and then select the bot permissions you need (for development, you can select <code>Administrator</code>).</li> <li>Go to the generated URL and invite the application to the desired server.</li> </ol>","tags":["webgroup","discord"]},{"location":"webgroup/blockbot/#bot-token","title":"Bot Token","text":"<ol> <li>Open the application on the Discord developer portal.</li> <li>Go to \"Bot\" on the left sidebar and click <code>Reset Token</code>.</li> <li>Copy the newly generated token.</li> </ol>","tags":["webgroup","discord"]},{"location":"webgroup/blockbot/#running-with-docker-compose-recommended","title":"Running with Docker Compose (recommended)","text":"<ol> <li> <p>Fork, <code>git clone</code> and <code>cd</code> into the blockbot repository.</p> <p>Tip</p> <p>Read the contributing docs for more information on using Git and GitHub.</p> </li> <li> <p>Rename <code>.env.sample</code> to <code>.env</code> inside the repo folder and fill in the environment variables with your secrets. e.g.:</p> Text Only<pre><code>TOKEN=&lt;Discord bot token here&gt;\n</code></pre> </li> <li> <p>Run the bot: <code>docker compose up --build bot</code>     This will also start the database.</p> </li> </ol>","tags":["webgroup","discord"]},{"location":"webgroup/blockbot/#running-from-source-deprecated","title":"Running from source (deprecated)","text":"<p>Blockbot uses <code>uv</code> to manage dependencies and run the project.</p> <ol> <li> <p>Fork, <code>git clone</code> and <code>cd</code> into the blockbot repository.</p> <p>Tip</p> <p>Read the contributing docs for more information on using Git and GitHub.</p> </li> <li> <p>It is generally advised to work in a Python virtual environment:</p> Bash<pre><code>uv venv\nsource .venv/bin/activate\n</code></pre> </li> <li> <p>Rename <code>.env.sample</code> to <code>.env</code> inside the repo folder and fill in the environment variables with your secrets. e.g.:</p> Text Only<pre><code>TOKEN=&lt;Discord bot token here&gt;\n</code></pre> </li> <li> <p>Run <code>uv sync --frozen</code> to install the required packages.</p> </li> <li> <p>Start the bot by running <code>uv run -m src</code>.</p> <p>Note</p> <p>Currently a valid database connection is required for the bot to start. Therefore we recommend running Blockbot with docker compose (the compose file includes a PostgreSQL service).</p> </li> </ol>","tags":["webgroup","discord"]},{"location":"webgroup/blockbot/#contributing-tools","title":"Contributing Tools","text":"<p>Blockbot adheres to various code styling and typing rules (listed under <code>[tool.ruff.format]</code> and <code>[tool.ruff.lint]</code> in <code>pyproject.toml</code>). To make sure you're following these rules when developing Blockbot, we use <code>nox</code>.</p> <p><code>nox</code> is configured in a <code>noxfile.py</code> file, located here for Blockbot.</p> <ol> <li> <p>Install <code>nox</code></p> Bash<pre><code>uv sync --group nox --frozen\n</code></pre> </li> <li> <p>Run <code>nox</code></p> Bash<pre><code>uv run nox\n</code></pre> <p>To run a specific session (e.g. <code>format_fix</code>): </p>Bash<pre><code>uv run nox -s format_fix\n</code></pre><p></p> </li> <li> <p>Fix any issues <code>nox</code> reports. This could be:</p> <ul> <li>code style issues, most of which <code>ruff</code> (the code linter/formatter) will try to fix automatically</li> <li>typing issues found by <code>pyright</code>. These will need to be fixed manually.</li> </ul> <p>Contact the webmaster if you need assistance fixing any issues!</p> </li> </ol>","tags":["webgroup","discord"]},{"location":"webgroup/blockbot/#library-resources","title":"Library Resources","text":"<ul> <li><code>hikari</code> Documentation &amp; Examples</li> <li><code>hikari-arc</code> Documentation &amp; Examples</li> <li><code>hikari-miru</code> Documentation &amp; Examples</li> </ul>","tags":["webgroup","discord"]},{"location":"webgroup/blockbot/#usage-guides","title":"Usage Guides","text":"","tags":["webgroup","discord"]},{"location":"webgroup/blockbot/#hikari","title":"hikari","text":"<ul> <li>Getting Started - first steps of using <code>hikari</code></li> <li>Events - understanding receiving events from Discord with <code>hikari</code></li> </ul>","tags":["webgroup","discord"]},{"location":"webgroup/blockbot/#hikari-arc","title":"hikari-arc","text":"<ul> <li>Getting Started - first steps of using <code>hikari-arc</code></li> <li>Guides - various guides on aspects of <code>hikari-arc</code></li> </ul>","tags":["webgroup","discord"]},{"location":"webgroup/blockbot/#hikari-miru","title":"hikari-miru","text":"<ul> <li>Getting Started - first steps of using <code>hikari-miru</code></li> <li>Guides - various guides on aspects of <code>hikari-miru</code></li> </ul>","tags":["webgroup","discord"]},{"location":"webgroup/blockbot/#faq","title":"FAQ","text":"","tags":["webgroup","discord"]},{"location":"webgroup/blockbot/#whats-the-difference-between-hikari-hikari-arc-and-hikari-miru","title":"What's the difference between <code>hikari</code>, <code>hikari-arc</code> and <code>hikari-miru</code>?","text":"<ul> <li><code>hikari</code> -  the Discord API wrapper. Can be used to, for example:<ul> <li>create threads</li> <li>send individual messages</li> <li>fetch guild (server) information</li> <li>update member roles (add role, remove role, edit roles)</li> <li>listen for events from Discord, like message edits</li> </ul> </li> <li><code>hikari-arc</code> - the command handler. Can be used to:<ul> <li>create application commands (slash, message &amp; user commands)</li> <li>respond to command interactions</li> </ul> </li> <li><code>hikari-miru</code> - the component handler. Can be used to:<ul> <li>create message components (buttons, select menus &amp; modals)</li> <li>respond to component interactions (button clicks, select menu selections &amp; modal submissions)</li> </ul> </li> </ul>","tags":["webgroup","discord"]},{"location":"webgroup/blockbot/#why-use-a-hikarigatewaybot-instead-of-a-hikarirestbot","title":"Why use a <code>hikari.GatewayBot</code> instead of a <code>hikari.RESTBot</code>?","text":"<p>TL;DR: <code>RESTBot</code>s do not receive events required for some blockbot features (e.g. starboard), so <code>GatewayBot</code> must be used instead.</p> <p><code>GatewayBot</code>s connect to Discord via a websocket, and Discord sends events (including interactions) through this websocket. <code>RESTBot</code>s run a web server which Discord sends only interactions to (not events) via HTTP requests. These events are required for specific blockbot features, like starboard (which uses reaction create/remove events).</p> <p>Further reading: https://arc.hypergonial.com/getting_started/#difference-between-gatewaybot-restbot</p>","tags":["webgroup","discord"]},{"location":"webgroup/blockbot/#whats-the-difference-between-hikarigatewaybot-arcgatewayclient-and-miruclient","title":"What's the difference between <code>hikari.GatewayBot</code>, <code>arc.GatewayClient</code> and <code>miru.Client</code>?","text":"<ul> <li><code>hikari.GatewayBot</code> is the actual Discord bot. It:<ul> <li>manages the websocket connection to Discord</li> <li>sends HTTP requests to the Discord REST API</li> <li>caches information received in events sent by Discord</li> </ul> </li> <li><code>arc.GatewayClient</code> adds additional functionality to <code>hikari.GatewayBot</code> for:<ul> <li>splitting the bot across multiple files using extensions</li> <li>grouping commands using plugins</li> <li>easily creating and managing commands and command groups</li> <li>accessing objects globally (in any extension, plugin or command) using dependency injection (e.g. a database connection)</li> </ul> </li> <li><code>miru.Client</code> adds additional functionality to <code>hikari.GatewayBot</code> for:<ul> <li>creating message components using views</li> <li>creating modals</li> <li>creating navigators and menus using views</li> </ul> </li> </ul>","tags":["webgroup","discord"]},{"location":"webgroup/blockbot/#dos-and-donts","title":"Do's and Don'ts","text":"<ul> <li> <p>Always try to get data from the cache before fetching it from the API.</p> Python<pre><code># command example\nasync def command(ctx: arc.GatewayContext) -&gt; None:\n    user = ctx.client.cache.get_user(123)\n    if not user:\n        user = await ctx.client.rest.fetch_user(123)\n</code></pre> </li> </ul>","tags":["webgroup","discord"]},{"location":"webgroup/contributing/","title":"Contributing to Webgroup","text":"","tags":["webgroup"]},{"location":"webgroup/contributing/#contributing-to-webgroup","title":"Contributing to Webgroup","text":"","tags":["webgroup"]},{"location":"webgroup/contributing/#basic-contribution-workflow","title":"Basic Contribution Workflow","text":"<ul> <li>Fork the repository on GitHub.</li> <li> <p>Clone your fork of the repository.</p> Bash<pre><code>git clone &lt;your fork URL&gt;\n</code></pre> </li> <li> <p>Switch to a new branch (name it appropriately!)</p> Bash<pre><code>git checkout -b &lt;new branch name&gt;\n</code></pre> </li> <li> <p>Make changes to the codebase.</p> <p>Note</p> <p>You don't have to make all the necessary changes in one commit. It's much better to split a bigger pull request over multiple commits. This will make it easier to manage and review.</p> </li> <li> <p>Stage and commit the changes.</p> Bash<pre><code>git add &lt;files you changed&gt;\ngit commit -m \"&lt;commit message&gt;\"\n</code></pre> <p>Tip</p> <p>See Writing Meaningful Commit Messages</p> </li> <li> <p>On GitHub, navigate to your fork repository and switch to the branch you created.</p> <p></p> </li> <li> <p>Use the \" Contribute\" button to open the pull request page.</p> <p></p> </li> <li> <p>Fill in the title and description fields, then open a pull request. If you have not finished making all the necessary changes, then open a draft pull request instead.</p> <p></p> <p>Tip</p> <p>For bigger contributions, it's advisable to open a draft pull request when you begin development so other maintainers (e.g. other members of webgroup) can review your changes and provide feedback as you work.</p> </li> </ul>","tags":["webgroup"]}]}